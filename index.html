<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Modelagem Ac√∫stica e Estima√ß√£o de Par√¢metros</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>


    <header>
        <h1>Modelagem Ac√∫stica, Processamento de Sinais de √Åudio e Processamento de Imagens</h1>
        <nav>
         <div class="language-switch">
            <a href="index-en.html">üá¨üáß EN</a>
            <a href="index-de.html">üá©üá™ DE</a>
         </div>
          <ul>
            <li><a href="#sobre">Sobre</a></li>
            <li><a href="#dissertacao">Disserta√ß√£o</a></li>
            <li><a href="#bibliografia">Bibliografia</a></li> <!-- Link para a se√ß√£o Bibliografia -->
            <li><a href="#projetos">Projetos</a></li>
            <li><a href="#api-estimacao">API para estima√ß√£o</a></li> <!-- Nova aba adicionada -->
            <li><a href="#artigos">Artigos publicados</a></li>
            <li><a href="#livros">Livros recomendados</a></li> <!-- Link para a se√ß√£o Livros Recomendados -->
            <li><a href="#contato">Contato</a></li>
          </ul>
        </nav>
    </header>

    <section id="sobre">
        <h2>Sobre</h2>
        <p>Ol√°! Meu nome √© Luis Paulo Albuquerque Guedes. Sou aluno de mestrado em Engenharia El√©trica, na √°rea de Processamento de Sinais de √Åudio, desenvolvida na COPPE/UFRJ, sob orienta√ß√£o da Professora Mariane Petr√°glia e do Professor Julio Cesar Boscher Torres.   
    Inicialmente, este reposit√≥rio tem como objetivo reunir os registros de progresso da minha disserta√ß√£o de mestrado.</p>

        <p>Meu curriculum vitae encontra-se dispon√≠vel em: <a href="https://drive.google.com/uc?export=download&id=1bxuK9c2iCFkGVn5Ir6R0GLqEurPVsBQe" download><button>Baixar CV</button></a></p>
    </section>

    <section id="dissertacao">
        <h2>Disserta√ß√£o</h2>
        <p>Confira os detalhes da vers√£o final da disserta√ß√£o: <a href="link-da-dissertacao" target="_blank">Acesse aqui</a>.</p>
    </section>

    <section id="bibliografia">
         <h2>Bibliografia</h2>
         <p>Confira a bibliografia utilizada para a disserta√ß√£o:</p>
         <ul>
             <li><a href="link-bibliografia-1" target="_blank">Refer√™ncia Bibliogr√°fica 1</a></li>
             <li><a href="link-bibliografia-2" target="_blank">Refer√™ncia Bibliogr√°fica 2</a></li>
             <li><a href="link-bibliografia-3" target="_blank">Refer√™ncia Bibliogr√°fica 3</a></li>
        <!-- Adicione mais refer√™ncias conforme necess√°rio -->
           </ul>
    </section>

    <style>
        ul {
            padding-left: 0;
            list-style-position: inside;
        }
    </style>

    <section id="projetos">
        <h2>Meus Projetos</h2>
        <p>Trata-se este reposit√≥rio dos registros do progresso das simula√ß√µes ac√∫sticas da disserta√ß√£o de mestrado sobre estima√ß√£o de par√¢metros ac√∫sticos usando modelos de Aprendizagem Profunda (Deep Learning).  </p>
        
          <!-- Espa√ßo reservado para Projeto 3 -->
        <div class="projeto">
            <h3><a href="link-projeto-3" target="_blank">Projeto 3</a></h3>
            <p>
              O Projeto 3 d√° continuidade ao Projeto 2, expandindo sua abordagem com a implementa√ß√£o de uma an√°lise binaural...
            </p>
            <p>Mais informa√ß√µes podem ser encontradas no seguinte link: <a href="link-resultados-3" target="_blank">Resultados do Projeto 3</a>.</p>
        </div>


        <!-- Projeto 2 -->
        <div class="projeto">
            <h3><a href="link-projeto-2" target="_blank">Projeto 2</a></h3>
            <p>
                ALTERA√á√ïES IMPLEMENTADAS:
                
                A ETAPA I: CONFIGURA√á√ÉO DOS AMBIENTES; ETAPA II: GERA√á√ÉO DE RESPOSTAS AO IMPULSO DE SALAS (RIS); ETAPA III: IMPLEMENTA√á√ÉO E TREINAMENTO DOS MODELOS; ETAPA III: TESTE DOS MODELOS; ETAPA IV: AVALIA√á√ÉO DAS M√âTRICAS DOS MODELOS.
            <ul>
               <li>ETAPA I: CONFIGURA√á√ÉO DOS AMBIENTES</li>
            </ul>  
            
            <ul>
               <li>PASSO 1: GERANDO AS CONFIGURA√á√ïES DA SALA</li>
            </ul>   
                Inicialmente, as dimens√µes das salas foram definidas aleatoriamente dentro de um intervalo preestabelecido. O comprimento e a largura variaram entre 3 e 12 metros, enquanto a altura assumiu valores entre 2,5 e 4,5 metros. Esses valores foram arredondados para uma casa decimal para garantir maior precis√£o nas simula√ß√µes.
                Al√©m das dimens√µes, foram atribu√≠dos coeficientes de difus√£o para cada superf√≠cie da sala. O mesmo valor foi aplicado uniformemente a todas as superf√≠cies e foi sorteado dentro de um intervalo entre 0,2 e 1. Esse coeficiente foi gerado para seis superf√≠cies distintas: quatro paredes, teto e ch√£o. Os valores foram armazenados de acordo com a frequ√™ncia correspondente, considerando seis faixas padronizadas: 125 Hz, 250 Hz, 500 Hz, 1000 Hz, 2000 Hz e 4000 Hz.
                Para determinar os coeficientes de absor√ß√£o, com o fim de definir perfis distintos para paredes, teto e ch√£o, foi utilizado um processo probabil√≠stico apresentado em:  <a href="https://drive.google.com/file/d/11VNCIks217v6AZXI_0JtuiTwnnlTSDv-/view?usp=drive_link" download><button>Mean Absorption Estimation from Room Impulse Responses using Virtually-Supervised Learning</button></a></p> Em alguns casos, um √∫nico valor foi atribu√≠do a todas as superf√≠cies de um mesmo tipo, enquanto em outros, valores diferenciados foram gerados para cada frequ√™ncia. Esses coeficientes foram estabelecidos a partir de intervalos que variaram conforme o tipo de superf√≠cie, refletindo as propriedades ac√∫sticas de materiais comuns.
                As paredes puderam apresentar coeficientes de absor√ß√£o homog√™neos ou diferenciados por frequ√™ncia, variando entre valores baixos, t√≠picos de materiais reflexivos, e valores mais elevados, condizentes com superf√≠cies mais absorventes. O teto e o ch√£o seguiram uma l√≥gica semelhante, com intervalos de absor√ß√£o distintos de acordo com materiais t√≠picos desses elementos estruturais.
                Dessa forma, o c√≥digo gerou automaticamente um conjunto de par√¢metros realistas para diferentes configura√ß√µes de salas, permitindo a simula√ß√£o de ambientes ac√∫sticos diversos com base nas propriedades f√≠sicas de suas superf√≠cies.
             
            <ul>
                <img src="room_3D.png" alt="3D Room" class="responsive-image">
                <figcaption>Visualiza√ß√£o 3D de uma amostra de sala simulada</figcaption>
            </ul>
             
            
            <ul>
               <li>PASSO 2: GERANDO AS CONFIGURA√á√ïES DOS RECEPTORES </li>
            </ul>
               O c√≥digo define uma classe chamada 'util_receiver', respons√°vel por gerar e visualizar a disposi√ß√£o de receptores e microfones em um ambiente tridimensional. Primeiramente, s√£o inicializados atributos para armazenar as informa√ß√µes dos receptores. Em seguida, para cada receptor, √© gerado um ponto central (baricentro) dentro de um espa√ßo tridimensional, 
               garantindo que esteja dentro dos limites definidos pela sala. A partir desse baricentro, s√£o calculadas as posi√ß√µes de dois microfones simetricamente distribu√≠dos ao longo do eixo X, garantindo um espa√ßamento fixo entre eles. Um vetor de dire√ß√£o perpendicular ao eixo que une os microfones √© ent√£o gerado, permitindo que a orienta√ß√£o do receptor varie 
               dentro do plano XY. Ap√≥s a gera√ß√£o dos receptores, microfones e vetores direcionais, um gr√°fico tridimensional interativo √© constru√≠do utilizando a biblioteca Plotly. O layout do gr√°fico √© configurado para exibir os eixos e uma legenda, proporcionando uma visualiza√ß√£o clara da distribui√ß√£o dos elementos no espa√ßo tridimensional.

            <ul>
               <li>PASSO 3: GERANDO AS CONFIGURA√á√ïES DAS FONTES DE FALA </li>
            </ul>
               O c√≥digo define uma classe chamada 'util_source', respons√°vel por gerar e posicionar fontes de som dentro de um ambiente tridimensional. Inicialmente, s√£o listados diferentes tipos de fontes sonoras, como omnidirecional, cardioide e bidirecional. Para criar fontes reais na sala, s√£o geradas coordenadas aleat√≥rias garantindo uma dist√¢ncia m√≠nima segura em rela√ß√£o a um ponto de refer√™ncia (baricentro). 
               A primeira fonte sempre ser√° omnidirecional, enquanto as demais s√£o atribu√≠das aleatoriamente a uma das categorias dispon√≠veis. Al√©m disso, cada fonte recebe um vetor de dire√ß√£o, que pode ser perpendicular ao plano horizontal ou gerado aleatoriamente.
               No caso das fontes de ru√≠do, s√£o definidos at√© cinco pontos estrat√©gicos, incluindo posi√ß√µes fixas no centro superior da sala e ao longo das extremidades, enquanto fontes adicionais s√£o posicionadas aleatoriamente dentro dos limites da sala. As dire√ß√µes dessas fontes de ru√≠do podem ser fixas (por exemplo, apontando para baixo no eixo Z) ou geradas aleatoriamente. Cada fonte recebe um √¢ngulo de orienta√ß√£o (yaw) aleat√≥rio dentro de um intervalo de -180 a 180 graus para garantir diversidade na orienta√ß√£o espacial. 
               Para evitar sobreposi√ß√µes ou posicionamentos inadequados, s√£o utilizadas verifica√ß√µes de dist√¢ncia m√≠nima entre as fontes e seus respectivos baricentros. Al√©m disso, vetores perpendiculares s√£o calculados para manter coer√™ncia na orienta√ß√£o das fontes, garantindo que suas dire√ß√µes estejam bem distribu√≠das dentro do espa√ßo tridimensional da sala.

            <ul>
               <li>PASSO 4: GERANDO AS CONFIGURA√á√ïES DAS FONTES DE RU√çDO </li>
            </ul>

              O c√≥digo gera e visualiza fontes de ru√≠do dentro de uma sala tridimensional simulada, associando cada fonte a um arquivo de som espec√≠fico e a uma √°rea de atua√ß√£o realista. A classe util_source posiciona estrategicamente os ru√≠dos no ambiente, garantindo que sua distribui√ß√£o reflita situa√ß√µes reais. Cada ru√≠do possui um intervalo de posicionamento definido no c√≥digo, assegurando coer√™ncia espacial com sua fonte original.
              Os ru√≠dos utilizados incluem ar-condicionado, tosse, risada, telefone tocando e digita√ß√£o no teclado, extra√≠dos do conjunto DCASE 2016 Task 2, dispon√≠veis em: <a href="https://dcase.community/challenge2016/task-sound-event-detection-in-synthetic-audio" download><button>Sound event detection in synthetic audio</button></a></p>  O ar-condicionado √© posicionado no teto, simulando um sistema de ventila√ß√£o. A tosse ocorre em altura m√©dia, representando uma pessoa no ambiente. 
              A risada √© colocada pr√≥xima √†s laterais, indicando conversas em grupo. O telefone aparece pr√≥ximo √†s extremidades da sala, em uma mesa ou esta√ß√£o de trabalho. J√° a digita√ß√£o √© posicionada pr√≥xima ao ch√£o, simulando teclados sobre superf√≠cies.
              A biblioteca Plotly √© usada para gerar um gr√°fico 3D interativo, onde cada fonte de ru√≠do √© representada por um marcador vermelho e vetores direcionais azuis indicam a propaga√ß√£o do som. Al√©m disso, as informa√ß√µes sobre posi√ß√£o, dire√ß√£o e arquivo de som correspondente s√£o organizadas em uma tabela formatada com a biblioteca tabulate.
              Essa abordagem garante que a simula√ß√£o seja realista e contextualizada, permitindo estudos avan√ßados sobre propaga√ß√£o ac√∫stica, interfer√™ncia sonora e inteligibilidade da fala em ambientes fechados. 
            
            <ul>
             <img src="room_sources.png" alt="3D Room" class="responsive-image">
             <figcaption>Visualiza√ß√£o 3D de uma amostra de sala simulada com microfones, fontes de fala e de ru√≠do</figcaption>
            </ul>
            
            <ul>
               <li>PASSO 5: COMPILANDO AS INFORMA√á√ïES</li>
            </ul>
               O c√≥digo define uma classe chamada conf_files, respons√°vel por gerar configura√ß√µes de salas para simula√ß√µes ac√∫sticas. Ao ser inicializada, a classe recebe o n√∫mero de salas a serem geradas e o nome do conjunto de dados. Em seguida, s√£o instanciadas as classes auxiliares respons√°veis pela defini√ß√£o dos receptores, fontes sonoras e propriedades da sala. Um diret√≥rio de armazenamento √© criado para salvar os arquivos de configura√ß√£o.
               Inicialmente, s√£o gerados os par√¢metros globais da simula√ß√£o, incluindo a taxa de amostragem, a dura√ß√£o da resposta ao impulso, a ordem de reflex√£o e a quantidade de raios para o tra√ßado estoc√°stico. Esses par√¢metros s√£o armazenados em arquivos nos formatos YAML e JSON. Na etapa de cria√ß√£o das salas, um dicion√°rio √© estruturado para armazenar informa√ß√µes sobre as dimens√µes da sala, coeficientes de absor√ß√£o e difus√£o, al√©m das condi√ß√µes ambientais, 
               como umidade e temperatura. Para cada sala, s√£o geradas posi√ß√µes de receptores e suas respectivas dire√ß√µes, assegurando um espa√ßamento adequado. Fontes sonoras tamb√©m s√£o distribu√≠das dentro do ambiente, garantindo que pelo menos uma delas seja omnidirecional. Al√©m disso, fontes de ru√≠do s√£o posicionadas em locais espec√≠ficos, como pr√≥ximos ao teto, lateralmente e perto do ch√£o, simulando diferentes condi√ß√µes ac√∫sticas.
               Ao final do processo, os dados estruturados s√£o armazenados em arquivos YAML e JSON, organizando as informa√ß√µes de cada sala, incluindo os par√¢metros de simula√ß√£o, receptores, fontes sonoras e fontes de ru√≠do. O progresso da gera√ß√£o das salas √© monitorado com uma barra de progresso para indicar a conclus√£o das itera√ß√µes.
            
             <ul>
               <li>PASSO 6: GERANDO E ARMAZENADOS AS SALAS</li>
             </ul>
                O c√≥digo cria 1000 salas e salva suas configura√ß√µes nos formatos YAML e JSON. Para cada sala, s√£o geradas dimens√µes, receptores, fontes sonoras (incluindo uma omnidirecional) e fontes de ru√≠do posicionadas estrategicamente. Os dados s√£o armazenados em arquivos organizados, e o progresso √© exibido com tqdm.
             
             <ul>
               <li>ETAPA II: GERA√á√ÉO DE RESPOSTAS AO IMPULSO DE SALAS (RIS)</li>
             </ul>  

                A segunda etapa teve como objetivo a implementa√ß√£o da biblioteca gpuRIR, uma ferramenta de c√≥digo aberto em Python para a simula√ß√£o de Respostas ao Impulso de Salas (RIRs). Essa biblioteca utiliza o Image Source Method (ISM) com acelera√ß√£o via GPU, 
                permitindo o c√°lculo eficiente de RIRs entre m√∫ltiplas fontes e receptores em paralelo por meio de GPUs CUDA. Comparada √†s implementa√ß√µes tradicionais baseadas em CPU, a gpuRIR oferece um desempenho significativamente superior, sendo aproximadamente 100 vezes mais r√°pida. A biblioteca gpuRIR est√° dispon√≠vel em: <a href="https://github.com/DavidDiazGuerra/gpuRIR" download><button>gpuRIR</button></a>
                
              <ul>
                O artigo que aborda a biblioteca gpuRIR pode ser encontrado em:  <a href="https://link.springer.com/article/10.1007/s11042-020-09905-3" download><button>gpuRIR: A python library for room impulse response simulation with GPU acceleration</button></a>
              </ul> 
              <ul>
              <img src="gpuRIR.png" alt="3D Room" class="responsive-image">
              <figcaption>Biblioteca gpuRIR</figcaption>
              </ul>

              <ul>
                A biblioteca gpuRIR gera RIRs para diferentes configura√ß√µes ac√∫sticas utilizando a biblioteca gpuRIR. Primeiramente, ele carrega os dados das salas, fontes sonoras e receptores a partir de arquivos Parquet. Com base nas dimens√µes da sala, coeficientes de absor√ß√£o e par√¢metros ac√∫sticos como T60, C50 e DRR, o c√≥digo calcula os coeficientes de reflex√£o das superf√≠cies e determina a ordem de imagem necess√°ria para a simula√ß√£o. Em seguida, para cada frequ√™ncia especificada (125, 250, 500, 1000, 2000 e 4000 Hz), s√£o geradas RIRs para todas as fontes sonoras, 
                levando em considera√ß√£o o padr√£o direcional das fontes e microfones. As RIRs s√£o simuladas separadamente para dois conjuntos de microfones e, ao final do processamento, os resultados s√£o armazenados em arquivos Parquet organizados por frequ√™ncia. O c√≥digo tamb√©m exibe o n√∫mero de fontes e receptores presentes na sala selecionada antes de finalizar a execu√ß√£o.
              </ul>

              <ul>
              <img src="RIR_sources.png" alt="3D Room" class="responsive-image">
              <figcaption>RIRs geradas para fontes de fala</figcaption>
              </ul>

              <ul>
              <img src="RIR_noises.png" alt="3D Room" class="responsive-image">
              <figcaption>RIRs geradas para fontes de ru√≠do</figcaption>
              </ul>
            
            
    
                Com o objetivo final de desenvolver uma abordagem para a estimativa cega de par√¢metros ac√∫sticos, este est√°gio inicial, de natureza experimental e explorat√≥ria, consistiu em implementar um modelo RESNET-50 para estimar as seguintes vari√°veis:
            </p>
            <ul>
               <li>√Årea Superficial (m¬≤)</li>
               <li>Volume (m¬≥)</li>
               <li>Superf√≠cie 1 (m¬≤)</li>
               <li>Superf√≠cie 2 (m¬≤)</li>
               <li>Superf√≠cie 3 (m¬≤)</li>
               <li>Superf√≠cie 4 (m¬≤)</li>
               <li>Superf√≠cie 5 (m¬≤)</li>
               <li>Superf√≠cie 6 (m¬≤)</li>
               <li>Absor√ß√£o M√©dia (125 Hz)</li>
               <li>Absor√ß√£o M√©dia (250 Hz)</li>
               <li>Absor√ß√£o M√©dia (500 Hz)</li>
               <li>Absor√ß√£o M√©dia (1000 Hz)</li>
               <li>Absor√ß√£o M√©dia (2000 Hz)</li>
               <li>Absor√ß√£o M√©dia (4000 Hz)</li>
               <li>T60_Sabine_125</li>
               <li>T60_Sabine_250</li>
               <li>T60_Sabine_500</li>
               <li>T60_Sabine_1000</li>
               <li>T60_Sabine_2000</li>
               <li>T60_Sabine_4000</li>
               <li>C50_125Hz</li>
               <li>T60_T30_125Hz</li>
               <li>C50_250Hz</li>
               <li>T60_T30_250Hz</li>
               <li>C50_500Hz</li>
               <li>T60_T30_500Hz</li>
               <li>C50_1000Hz</li>
               <li>T60_T30_1000Hz</li>
               <li>C50_2000Hz</li>
               <li>T60_T30_2000Hz</li>
               <li>C50_4000Hz</li>
               <li>T60_T30_4000Hz</li>
            </ul>
            
            <p>O resultado preliminar encontra-se no seguinte link: <a href="link-resultados" target="_blank">Resultados</a>.</p>
            <p>O modelo treinado encontra-se no seguinte link: <a href="link-modelo" target="_blank">Modelo Treinado</a>.</p>
            
            <h4>Caracter√≠sticas do Modelo</h4>
            <ul>
                <li>Base: ResNet-50 pr√©-treinada (ImageNet).</li>
                <li>Camadas finais removidas (mantendo apenas a extra√ß√£o de caracter√≠sticas).</li>
                <li>Camadas adicionais: 
                    <ul>
                        <li>Global Average Pooling (reduzindo a sa√≠da para 2048 neur√¥nios).</li>
                        <li>Uma cabe√ßa para predi√ß√£o dos valores esperados (mu_head).</li>
                        <li>Uma cabe√ßa para predi√ß√£o da incerteza (sigma_head), ativada com Softplus.</li>
                    </ul>
                </li>
            </ul>

            <h4>Fun√ß√£o de Perda - Log-Verossimilhan√ßa Gaussiana</h4>
            <p>
                O modelo utiliza uma fun√ß√£o de perda baseada em distribui√ß√£o Gaussiana para estimar a incerteza da predi√ß√£o. A equa√ß√£o utilizada √©:
            </p>
            <pre>
            L = 0.5 * Œ£ (log(œÉ¬≤) + ((y - Œº)¬≤ / œÉ¬≤))
            </pre>
            <p>Essa abordagem permite que o modelo aprenda n√£o apenas os valores das vari√°veis ac√∫sticas, mas tamb√©m o n√≠vel de incerteza associado √†s predi√ß√µes.</p>

            <h4>Hiperpar√¢metros do Treinamento</h4>
            <ul>
                <li>√âpocas: 50</li>
                <li>Otimizador: Adam (lr=1e-5)</li>
                <li>Divis√£o dos Dados: 80% Treino, 20% Valida√ß√£o</li>
                <li>Checkpoint: Salvo a cada √©poca</li>
                <li>GPU utilizada: NVIDIA T4</li>
            </ul>

            <h4>Treinamento e Valida√ß√£o</h4>
            <p>
                Durante o treinamento, o modelo foi avaliado em cada √©poca utilizando a fun√ß√£o de perda de log-verossimilhan√ßa Gaussiana. O progresso do treinamento e valida√ß√£o foi registrado, gerando um gr√°fico de perdas ao longo das √©pocas.
            </p>
            <ul>
                <li>Acur√°cia no treino: X%</li>
                <li>Erro de treino: Y</li>
                <li>Erro de valida√ß√£o: Z</li>
                <li>Acur√°cia no teste: W%</li>
                <li>Erro no teste: V</li>
            </ul>

            <h4>Visualiza√ß√£o das Perdas</h4>
            <p>O gr√°fico abaixo mostra a evolu√ß√£o da perda durante o treinamento:</p>
            <img src="link-grafico" alt="Gr√°fico de Treinamento e Valida√ß√£o" style="width: 100%; max-width: 600px;">
        </div>

        <!-- Espa√ßo reservado para Projeto 1 -->
        <div class="projeto">
            <h3><a href="link-projeto-1" target="_blank">Projeto 1</a></h3>
            <p>
                A ETAPA I: CONFIGURA√á√ÉO DOS AMBIENTES; ETAPA II: IMPLEMENTA√á√ÉO E TREINAMENTO DOS MODELOS; ETAPA III: TESTE DOS MODELOS; ETAPA IV: AVALIA√á√ÉO DAS M√âTRICAS DOS MODELOS.
            <ul>
               <li>ETAPA I: CONFIGURA√á√ÉO DOS AMBIENTES</li>
            </ul>  
            
            <ul>
               <li>PASSO 1: GERANDO AS CONFIGURA√á√ïES DA SALA</li>
            </ul>   
                Inicialmente, as dimens√µes das salas foram definidas aleatoriamente dentro de um intervalo preestabelecido. O comprimento e a largura variaram entre 3 e 12 metros, enquanto a altura assumiu valores entre 2,5 e 4,5 metros. Esses valores foram arredondados para uma casa decimal para garantir maior precis√£o nas simula√ß√µes.
                Al√©m das dimens√µes, foram atribu√≠dos coeficientes de difus√£o para cada superf√≠cie da sala. O mesmo valor foi aplicado uniformemente a todas as superf√≠cies e foi sorteado dentro de um intervalo entre 0,2 e 1. Esse coeficiente foi gerado para seis superf√≠cies distintas: quatro paredes, teto e ch√£o. Os valores foram armazenados de acordo com a frequ√™ncia correspondente, considerando seis faixas padronizadas: 125 Hz, 250 Hz, 500 Hz, 1000 Hz, 2000 Hz e 4000 Hz.
                Para determinar os coeficientes de absor√ß√£o, com o fim de definir perfis distintos para paredes, teto e ch√£o, foi utilizado um processo probabil√≠stico apresentado em  <a href="https://drive.google.com/file/d/11VNCIks217v6AZXI_0JtuiTwnnlTSDv-/view?usp=drive_link" download><button>Mean Absorption Estimation from Room Impulse Responses using Virtually-Supervised Learning</button></a></p> Em alguns casos, um √∫nico valor foi atribu√≠do a todas as superf√≠cies de um mesmo tipo, enquanto em outros, valores diferenciados foram gerados para cada frequ√™ncia. Esses coeficientes foram estabelecidos a partir de intervalos que variaram conforme o tipo de superf√≠cie, refletindo as propriedades ac√∫sticas de materiais comuns.
                As paredes puderam apresentar coeficientes de absor√ß√£o homog√™neos ou diferenciados por frequ√™ncia, variando entre valores baixos, t√≠picos de materiais reflexivos, e valores mais elevados, condizentes com superf√≠cies mais absorventes. O teto e o ch√£o seguiram uma l√≥gica semelhante, com intervalos de absor√ß√£o distintos de acordo com materiais t√≠picos desses elementos estruturais.
                Dessa forma, o c√≥digo gerou automaticamente um conjunto de par√¢metros realistas para diferentes configura√ß√µes de salas, permitindo a simula√ß√£o de ambientes ac√∫sticos diversos com base nas propriedades f√≠sicas de suas superf√≠cies.

             <ul>
               <li>PASSO 2: GERANDO AS CONFIGURA√á√ïES DOS RECEPTORES </li>
            </ul>
                A classe 'util_receiver' √© respons√°vel por definir e gerar arranjos de microfones em um ambiente tridimensional. Inicialmente, ela armazena os receptores em um dicion√°rio ('dic_receiver'). O m√©todo 'mic_defination_array' calcula as posi√ß√µes de dois microfones a partir de um ponto central (barycenter), aplicando uma matriz de rota√ß√£o para posicion√°-los a uma dist√¢ncia fixa de 22.5 cm. O m√©todo generate_receivers_rooms distribui aleatoriamente um conjunto de receptores dentro de uma sala, garantindo uma dist√¢ncia m√≠nima das paredes, al√©m de gerar √¢ngulos aleat√≥rios de orienta√ß√£o (yaw, pitch e roll). Ele utiliza o m√©todo 'rotation_matrix', que constr√≥i uma matriz de rota√ß√£o tridimensional com base nesses √¢ngulos, 
                permitindo a correta orienta√ß√£o dos microfones no espa√ßo. Os resultados incluem as posi√ß√µes dos microfones, os barycenters e os √¢ngulos de rota√ß√£o, tornando essa classe √∫til para simula√ß√µes ac√∫sticas e an√°lise de propaga√ß√£o sonora.

             <ul>
               <li>PASSO 3: GERANDO AS CONFIGURA√á√ïES DAS FONTES</li>
             </ul>
                A classe 'util_source' √© respons√°vel por gerar fontes sonoras dentro de uma sala tridimensional, garantindo posi√ß√µes seguras e evitando sobreposi√ß√£o com o barycenter. Ela cont√©m uma lista de descri√ß√µes de padr√µes de capta√ß√£o de microfones, incluindo op√ß√µes como omnidirecional, cardi√≥ide e bidirecional. O m√©todo 'generate_source_room' cria fontes reais na sala, garantindo que pelo menos uma delas seja omnidirecional 
                e posicionando as demais aleatoriamente com diferentes orienta√ß√µes (yaw, pitch e roll). O m√©todo 'fake_source_room' realiza um processo semelhante para fontes artificiais, garantindo uma dist√¢ncia m√≠nima entre elas e o barycenter. Ambos os m√©todos utilizam 'generate_safe_coordinates', que assegura que as fontes estejam dentro dos limites da sala e respeitem uma dist√¢ncia m√≠nima do barycenter antes de serem posicionadas. 
                Essa abordagem permite a simula√ß√£o de fontes sonoras em diferentes condi√ß√µes espaciais para an√°lise ac√∫stica.
            <ul>
               <li>PASSO 4: COMPILANDO AS INFORMA√á√ïES</li>
             </ul>
               A classe 'conf_files' combina funcionalidades das classes 'util_room', 'util_receiver' e 'util_source' para gerar arquivos de configura√ß√£o que descrevem salas ac√∫sticas, receptores (microfones) e fontes sonoras. No m√©todo '__init__', ela recebe o n√∫mero de salas e o nome do conjunto de dados, inicializando inst√¢ncias auxiliares e chamando os m√©todos 'params_file()' e 'room_file()'. O m√©todo 'params_file()' gera um arquivo YAML 
               contendo os par√¢metros da simula√ß√£o, como taxa de amostragem (48 kHz), ordem de reflex√£o, e n√∫mero de raios para o modelo de tra√ßado de raios difuso. O m√©todo 'room_file()' cria dicion√°rios com as propriedades de cada sala, incluindo dimens√µes, coeficientes de absor√ß√£o e difus√£o, al√©m da distribui√ß√£o de receptores e fontes sonoras. Para cada sala, ele posiciona cinco receptores garantindo uma dist√¢ncia m√≠nima das paredes, gera fontes reais e artificiais, 
               e armazena as configura√ß√µes em arquivos YAML separados para salas, receptores, fontes e fontes artificiais de ru√≠do. A abordagem facilita a simula√ß√£o e an√°lise ac√∫stica automatizada.
            
             <ul>
               <li>PASSO 5: GERANDO E ARMAZENADOS AS SALAS</li>
             </ul>
               A chamada 'conf_files(1000, "test")' cria e configura automaticamente 1000 salas ac√∫sticas para simula√ß√£o, gerando arquivos YAML detalhados com par√¢metros da sala, receptores e fontes sonoras. Durante a execu√ß√£o, a classe define dimens√µes aleat√≥rias para cada sala, calcula coeficientes de absor√ß√£o e difus√£o, posiciona cinco receptores respeitando uma dist√¢ncia m√≠nima das paredes e distribui fontes sonoras reais e artificiais, garantindo ao menos uma omnidirecional. 
               Os dados s√£o organizados em arquivos YAML, incluindo conf_sim_params.yml para par√¢metros da simula√ß√£o, conf_room_setup_2.yml com detalhes das salas, conf_receivers_2.yml para microfones, conf_source_2.yml para fontes reais e conf_noise_source_2.yml para fontes artificiais, permitindo sua aplica√ß√£o em estudos de propaga√ß√£o sonora e modelagem ac√∫stica.
            
    </section>

    


    <section id="api-estimacao">
        <h2>API para Estima√ß√£o</h2>
             <p>Esta API fornece funcionalidades avan√ßadas para estimar par√¢metros ac√∫sticos e realizar processamento de sinais.</p>
             <p>Acesse a API clicando no link abaixo:</p>
             <a href="https://seu-link-da-api.com" target="_blank">Acesse a API para Estima√ß√£o</a>
    </section>


    

  
    
    <section id="artigos">
        <h2>Artigos Publicados</h2>
        <div class="artigo">
            <h3>Unreal Engine 5 Simulations of Solar Plant Inspections by Unmanned Aerial Systems with Robot Operating System 2</h3>
            <p><strong>Autores:</strong> Fabio Andrade, Agnar Sivertsen, Marcos G L Moura, Lucas Cavalcante Clarino, Gabriel Souza Machado Gonzal√©z, Luis Paulo Albuquerque Guedes, Carlos Alberto Correia, Mariane Petraglia, Alessandro Rosa Lopes Zachi </p>
            <p><strong>Resumo:</strong> Este artigo apresenta um sistema de simula√ß√£o de alta fidelidade para inspe√ß√£o de usinas solares utilizando Sistemas A√©reos N√£o Tripulados. Ele integra o Unreal Engine 5 para visuais realistas, o AirSim para simula√ß√£o da f√≠sica e sensores de drones e o Robot Operating System 2 para desenvolvimento de algoritmos. O sistema permite testar algoritmos de inspe√ß√£o em um ambiente virtual realista, aprimorando o desenvolvimento de solu√ß√µes de vis√£o computacional e detec√ß√£o de objetos. Um estudo de caso utilizando a Unidade de Controle de Voo ArduPilot, Detec√ß√£o de Bordas de Canny e controle PID demonstra sua efic√°cia.</p>
            <p><a href="link-artigo" target="_blank">Acesse o artigo completo</a></p>
        </div>
        
        <div class="artigo">
            <h3>Radio Frequency-Audio Based Drone Classification using Deep Learning Methods</h3>
            <p><strong>Autores:</strong> Luis Paulo Albuquerque Guedes, Mariane Petraglia, R√™mulo M. Caminha Gomes</p>
            <p><strong>Resumo:</strong> Este artigo prop√µe um modelo de aprendizado profundo que utiliza a fus√£o de dados de sensores ac√∫sticos e de radiofrequ√™ncia (RF) para a classifica√ß√£o de Ve√≠culos A√©reos N√£o Tripulados (UAVs). O modelo visa aumentar a precis√£o na classifica√ß√£o de diferentes tipos de drones, garantindo redund√¢ncia para evitar falhas. O trabalho foi estruturado em tr√™s etapas: an√°lise ac√∫stica para classifica√ß√£o de tipos de drones (etapa 1), classifica√ß√£o baseada em assinaturas de RF (etapa 2) e integra√ß√£o dos sensores (etapa 3). A combina√ß√£o de sensores ac√∫sticos e de RF resultou em uma melhoria significativa no desempenho da classifica√ß√£o de drones, aumentando a precis√£o de 0.8342 para 0.8617. Al√©m disso, o uso dessa fus√£o de dados melhorou a precis√£o em at√© 15,3% em faixas de SNR mais baixas.</p>
            <p><a href="link-artigo" target="_blank">Acesse o artigo completo</a></p>
        </div>
        
        <div class="artigo">
            <h3>An Integrated Framework for UAV Sound Tracking and Classification Using Deep Learning Techniques and Kalman Filters</h3>
            <p><strong>Autores:</strong> Luis Paulo Albuquerque Guedes, Mariane Petraglia, Pedro Henrique Monteiro Guedes</p>
            <p><strong>Resumo:</strong> Este artigo apresenta um framework integrado que combina classifica√ß√£o ac√∫stica e rastreamento de trajet√≥ria de Ve√≠culos A√©reos N√£o Tripulados (UAVs) utilizando t√©cnicas de aprendizado profundo e filtros de Kalman. A abordagem proposta visa melhorar a precis√£o na identifica√ß√£o do tipo de UAV e na predi√ß√£o de trajet√≥ria, com base em dados ac√∫sticos passivos reais registrados em uma c√¢mara anecoica e convolvidos com ru√≠do ambiental. Os resultados das simula√ß√µes confirmaram a efic√°cia do sistema tanto nas tarefas de classifica√ß√£o quanto na estimativa de trajet√≥ria. Para a classifica√ß√£o do tipo de UAV, o F1-score geral atingiu 0.8342, com o Drone C obtendo o melhor desempenho (0.8504). Para a classifica√ß√£o da dire√ß√£o de manobra, os Drones A e B alcan√ßaram um F1-score de 0.8277, enquanto o Drone C obteve 0.8298. O erro m√©dio de dist√¢ncia na estimativa de trajet√≥ria foi de aproximadamente 8 metros, com desvios padr√£o variando de 3.34 a 3.55 metros, demonstrando consist√™ncia e precis√£o em todas as dimens√µes avaliadas.</p>
            <p><a href="link-artigo" target="_blank">Acesse o artigo completo</a></p>
        </div>

     </section>

     <section id="livros">
        <h2>Livros Recomendados</h2>
        <p>Aqui est√£o alguns livros recomendados sobre modelagem ac√∫stica e processamento de sinais:</p>
        <ul>
            <li><a href="link-livro-1" target="_blank">Livro 1</a></li>
            <li><a href="link-livro-2" target="_blank">Livro 2</a></li>
            <li><a href="link-livro-3" target="_blank">Livro 3</a></li>
        <!-- Adicione mais livros conforme necess√°rio -->
        </ul>
     </section>
    

     <section id="contato">
        <h2>Contato</h2>
        <ul>
            <li>
                <img src="email-icon.png" alt="Email" width="20"> 
                <a href="mailto:luis.albuquerque@marinha.mil.br">luis.albuquerque@marinha.mil.br</a>
            </li>
            <li>
                <img src="email-icon.png" alt="Email" width="20"> 
                <a href="mailto:luis.guedes@coppe.ufrj.br">luis.guedes@coppe.ufrj.br</a>
            </li>
            <li>
                <img src="email-icon.png" alt="Email" width="20"> 
                <a href="mailto:luispauloaguedes@gmail.com">luispauloaguedes@gmail.com</a>
            </li>
            <li>
                <img src="github-icon.png" alt="GitHub" width="20"> 
                <a href="https://github.com/seuusuario" target="_blank">GitHub</a>
            </li>
            <li>
                <img src="linkedin-icon.png" alt="LinkedIn" width="20"> 
                <a href="https://www.linkedin.com/in/seuperfil" target="_blank">LinkedIn</a>
            </li>
        </ul>
    </section>

    <script>
        function translate(language) {
            if (language === 'en') {
                window.location.href = "pagina-ingles.html";
            } else if (language === 'de') {
                window.location.href = "pagina-alemao.html";
            }
        }
    </script>

    <script>
        function translate(language) {
            if (language === 'en') {
                window.location.href = "pagina-ingles.html";  // P√°gina em ingl√™s
            } else if (language === 'de') {
                window.location.href = "pagina-alemao.html";  // P√°gina em alem√£o
            }
        }
    </script>

</body>
</html>
