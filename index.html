<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Modelagem Acústica e Estimação de Parâmetros</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

 <!-- Cabeçalho e navegação -->
    <header>
        <h1></h1>
        <nav>
            <!-- Botões para trocar de idioma -->
            <button onclick="translate('en')">English</button>
            <button onclick="translate('de')">Deutsch</button>
        </nav>
    </header>

    <header>
        <h1>Modelagem Acústica, Processamento de Sinais de Áudio e Processamento de Imagens</h1>
        <nav>
          <ul>
            <li><a href="#sobre">Sobre</a></li>
            <li><a href="#dissertacao">Dissertação</a></li>
            <li><a href="#bibliografia">Bibliografia</a></li> <!-- Link para a seção Bibliografia -->
            <li><a href="#projetos">Projetos</a></li>
            <li><a href="#api-estimacao">API para estimação</a></li> <!-- Nova aba adicionada -->
            <li><a href="#artigos">Artigos publicados</a></li>
            <li><a href="#livros">Livros recomendados</a></li> <!-- Link para a seção Livros Recomendados -->
            <li><a href="#contato">Contato</a></li>
          </ul>
        </nav>
    </header>

    <section id="sobre">
        <h2>Sobre Mim</h2>
        <p>Olá! Meu nome é Luis Paulo Albuquerque Guedes, sou Capitão-Tenente da Marinha do Brasil. Sou bacharel em Ciências Navais pela Escola Naval (2015) e bacharel em Ciências Econômicas pela Universidade Católica de Brasília (2022). Tenho Especialização em Análise de Ambiente Eletromagnético pelo Instituto Tecnológico de Aeronáutica - ITA (2022) e em Sistemas de Defesa Eletrônica pela Pontifícia Universidade Católica do Rio de Janeiro - PUC-Rio (2021). Exerci função técnica de Encarregado da Seção de Planejamento e Prospecção em Guerra Acústica no Centro de Guerra Acústica e Eletrônica da Marinha (CGAEM), e funções operativas de Chefe de Departamento de Intendência, de Encarregado de Divisão de Sistemas de Armas e de Sistemas Eletrônicos, todas em navios da Marinha do Brasil. Atualmente, sou aluno de mestrado em Engenharia Elétrica, na área de Processamento de Sinais de Áudio, desenvolvida na COPPE/UFRJ, sob orientação da Professora Mariane Petráglia e do Professor Julio Cesar Boscher Torres.   
    Inicialmente, este repositório tem como objetivo reunir os registros de progresso da minha dissertação de mestrado.</p>

        <p>Meu curriculum vitae encontra-se disponível em: <a href="https://drive.google.com/uc?export=download&id=1bxuK9c2iCFkGVn5Ir6R0GLqEurPVsBQe" download><button>Baixar CV</button></a></p>
    </section>

    <section id="dissertacao">
        <h2>Dissertação</h2>
        <p>Confira os detalhes da versão final da dissertação: <a href="link-da-dissertacao" target="_blank">Acesse aqui</a>.</p>
    </section>

    <section id="bibliografia">
         <h2>Bibliografia</h2>
         <p>Confira a bibliografia utilizada para a dissertação:</p>
         <ul>
             <li><a href="link-bibliografia-1" target="_blank">Referência Bibliográfica 1</a></li>
             <li><a href="link-bibliografia-2" target="_blank">Referência Bibliográfica 2</a></li>
             <li><a href="link-bibliografia-3" target="_blank">Referência Bibliográfica 3</a></li>
        <!-- Adicione mais referências conforme necessário -->
           </ul>
    </section>

    <style>
        ul {
            padding-left: 0;
            list-style-position: inside;
        }
    </style>

    <section id="projetos">
        <h2>Meus Projetos</h2>
        <p>Trata-se este repositório dos registros do progresso das simulações acústicas da dissertação de mestrado sobre estimação de parâmetros acústicos usando modelos de Aprendizagem Profunda (Deep Learning).  </p>
        
          <!-- Espaço reservado para Projeto 3 -->
        <div class="projeto">
            <h3><a href="link-projeto-3" target="_blank">Projeto 3</a></h3>
            <p>
                Descrição do Projeto 3. Este espaço pode ser preenchido com detalhes sobre o terceiro projeto, explicando sua relevância, metodologia aplicada e conclusões alcançadas.
            </p>
            <p>Mais informações podem ser encontradas no seguinte link: <a href="link-resultados-3" target="_blank">Resultados do Projeto 3</a>.</p>
        </div>


        <!-- Projeto 2 -->
        <div class="projeto">
            <h3><a href="link-projeto-2" target="_blank">Projeto 2</a></h3>
            <p>
                ALTERAÇÕES IMPLEMENTADAS:
                
                A ETAPA I: CONFIGURAÇÃO DOS AMBIENTES; ETAPA II: IMPLEMENTAÇÃO E TREINAMENTO DOS MODELOS; ETAPA III: TESTE DOS MODELOS; ETAPA IV: AVALIAÇÃO DAS MÉTRICAS DOS MODELOS.
            <ul>
               <li>ETAPA I: CONFIGURAÇÃO DOS AMBIENTES</li>
            </ul>  
            
            <ul>
               <li>PASSO 1: GERANDO AS CONFIGURAÇÕES DA SALA</li>
            </ul>   
                Inicialmente, as dimensões das salas foram definidas aleatoriamente dentro de um intervalo preestabelecido. O comprimento e a largura variaram entre 3 e 12 metros, enquanto a altura assumiu valores entre 2,5 e 4,5 metros. Esses valores foram arredondados para uma casa decimal para garantir maior precisão nas simulações.
                Além das dimensões, foram atribuídos coeficientes de difusão para cada superfície da sala. O mesmo valor foi aplicado uniformemente a todas as superfícies e foi sorteado dentro de um intervalo entre 0,2 e 1. Esse coeficiente foi gerado para seis superfícies distintas: quatro paredes, teto e chão. Os valores foram armazenados de acordo com a frequência correspondente, considerando seis faixas padronizadas: 125 Hz, 250 Hz, 500 Hz, 1000 Hz, 2000 Hz e 4000 Hz.
                Para determinar os coeficientes de absorção, com o fim de definir perfis distintos para paredes, teto e chão, foi utilizado um processo probabilístico apresentado em  <a href="https://drive.google.com/file/d/11VNCIks217v6AZXI_0JtuiTwnnlTSDv-/view?usp=drive_link" download><button>Mean Absorption Estimation from Room Impulse Responses using Virtually-Supervised Learning</button></a></p> Em alguns casos, um único valor foi atribuído a todas as superfícies de um mesmo tipo, enquanto em outros, valores diferenciados foram gerados para cada frequência. Esses coeficientes foram estabelecidos a partir de intervalos que variaram conforme o tipo de superfície, refletindo as propriedades acústicas de materiais comuns.
                As paredes puderam apresentar coeficientes de absorção homogêneos ou diferenciados por frequência, variando entre valores baixos, típicos de materiais reflexivos, e valores mais elevados, condizentes com superfícies mais absorventes. O teto e o chão seguiram uma lógica semelhante, com intervalos de absorção distintos de acordo com materiais típicos desses elementos estruturais.
                Dessa forma, o código gerou automaticamente um conjunto de parâmetros realistas para diferentes configurações de salas, permitindo a simulação de ambientes acústicos diversos com base nas propriedades físicas de suas superfícies.

             <ul>
               <li>PASSO 2: GERANDO AS CONFIGURAÇÕES DOS RECEPTORES </li>
            </ul>
               O código define uma classe chamada 'util_receiver', responsável por gerar e visualizar a disposição de receptores e microfones em um ambiente tridimensional. Primeiramente, são inicializados atributos para armazenar as informações dos receptores. Em seguida, para cada receptor, é gerado um ponto central (baricentro) dentro de um espaço tridimensional, 
               garantindo que esteja dentro dos limites definidos pela sala. A partir desse baricentro, são calculadas as posições de dois microfones simetricamente distribuídos ao longo do eixo X, garantindo um espaçamento fixo entre eles. Um vetor de direção perpendicular ao eixo que une os microfones é então gerado, permitindo que a orientação do receptor varie 
               dentro do plano XY. Após a geração dos receptores, microfones e vetores direcionais, um gráfico tridimensional interativo é construído utilizando a biblioteca Plotly. O layout do gráfico é configurado para exibir os eixos e uma legenda, proporcionando uma visualização clara da distribuição dos elementos no espaço tridimensional.

            <ul>
               <li>PASSO 3: GERANDO AS CONFIGURAÇÕES DAS FONTES DE FALA </li>
            </ul>
               O código define uma classe chamada 'util_source', responsável por gerar e posicionar fontes de som dentro de um ambiente tridimensional. Inicialmente, são listados diferentes tipos de fontes sonoras, como omnidirecional, cardioide e bidirecional. Para criar fontes reais na sala, são geradas coordenadas aleatórias garantindo uma distância mínima segura em relação a um ponto de referência (baricentro). 
               A primeira fonte sempre será omnidirecional, enquanto as demais são atribuídas aleatoriamente a uma das categorias disponíveis. Além disso, cada fonte recebe um vetor de direção, que pode ser perpendicular ao plano horizontal ou gerado aleatoriamente.
               No caso das fontes de ruído, são definidos até cinco pontos estratégicos, incluindo posições fixas no centro superior da sala e ao longo das extremidades, enquanto fontes adicionais são posicionadas aleatoriamente dentro dos limites da sala. As direções dessas fontes de ruído podem ser fixas (por exemplo, apontando para baixo no eixo Z) ou geradas aleatoriamente. Cada fonte recebe um ângulo de orientação (yaw) aleatório dentro de um intervalo de -180 a 180 graus para garantir diversidade na orientação espacial. 
               Para evitar sobreposições ou posicionamentos inadequados, são utilizadas verificações de distância mínima entre as fontes e seus respectivos baricentros. Além disso, vetores perpendiculares são calculados para manter coerência na orientação das fontes, garantindo que suas direções estejam bem distribuídas dentro do espaço tridimensional da sala.

            <ul>
               <li>PASSO 4: GERANDO AS CONFIGURAÇÕES DAS FONTES DE RUÍDO </li>
            </ul>

              O código gera e visualiza fontes de ruído dentro de uma sala tridimensional simulada, associando cada fonte a um arquivo de som específico e a uma área de atuação realista. A classe util_source posiciona estrategicamente os ruídos no ambiente, garantindo que sua distribuição reflita situações reais. Cada ruído possui um intervalo de posicionamento definido no código, assegurando coerência espacial com sua fonte original.
              Os ruídos utilizados incluem ar-condicionado, tosse, risada, telefone tocando e digitação no teclado, extraídos do conjunto DCASE 2016 Task 2, disponíveis em: <a href="https://dcase.community/challenge2016/task-sound-event-detection-in-synthetic-audio" download><button>Sound event detection in synthetic audio</button></a></p>  O ar-condicionado é posicionado no teto, simulando um sistema de ventilação. A tosse ocorre em altura média, representando uma pessoa no ambiente. A risada é colocada próxima às laterais, indicando conversas em grupo. O telefone aparece próximo às extremidades da sala, em uma mesa ou estação de trabalho. Já a digitação é posicionada próxima ao chão, simulando teclados sobre superfícies.
              A biblioteca Plotly é usada para gerar um gráfico 3D interativo, onde cada fonte de ruído é representada por um marcador vermelho e vetores direcionais azuis indicam a propagação do som. Além disso, as informações sobre posição, direção e arquivo de som correspondente são organizadas em uma tabela formatada com a biblioteca tabulate.
              Essa abordagem garante que a simulação seja realista e contextualizada, permitindo estudos avançados sobre propagação acústica, interferência sonora e inteligibilidade da fala em ambientes fechados. 
            
            <ul>
               <li>PASSO 5: COMPILANDO AS INFORMAÇÕES</li>
            </ul>
               O código define uma classe chamada conf_files, responsável por gerar configurações de salas para simulações acústicas. Ao ser inicializada, a classe recebe o número de salas a serem geradas e o nome do conjunto de dados. Em seguida, são instanciadas as classes auxiliares responsáveis pela definição dos receptores, fontes sonoras e propriedades da sala. Um diretório de armazenamento é criado para salvar os arquivos de configuração.
               Inicialmente, são gerados os parâmetros globais da simulação, incluindo a taxa de amostragem, a duração da resposta ao impulso, a ordem de reflexão e a quantidade de raios para o traçado estocástico. Esses parâmetros são armazenados em arquivos nos formatos YAML e JSON. Na etapa de criação das salas, um dicionário é estruturado para armazenar informações sobre as dimensões da sala, coeficientes de absorção e difusão, além das condições ambientais, 
               como umidade e temperatura. Para cada sala, são geradas posições de receptores e suas respectivas direções, assegurando um espaçamento adequado. Fontes sonoras também são distribuídas dentro do ambiente, garantindo que pelo menos uma delas seja omnidirecional. Além disso, fontes de ruído são posicionadas em locais específicos, como próximos ao teto, lateralmente e perto do chão, simulando diferentes condições acústicas.
               Ao final do processo, os dados estruturados são armazenados em arquivos YAML e JSON, organizando as informações de cada sala, incluindo os parâmetros de simulação, receptores, fontes sonoras e fontes de ruído. O progresso da geração das salas é monitorado com uma barra de progresso para indicar a conclusão das iterações.
            
             <ul>
               <li>PASSO 6: GERANDO E ARMAZENADOS AS SALAS</li>
             </ul>
                O código cria 1000 salas e salva suas configurações nos formatos YAML e JSON. Para cada sala, são geradas dimensões, receptores, fontes sonoras (incluindo uma omnidirecional) e fontes de ruído posicionadas estrategicamente. Os dados são armazenados em arquivos organizados, e o progresso é exibido com tqdm.

             <img src="https://drive.google.com/uc?export=view&id=1ZxLomOL2De4cV97EkrISiN1FHfXuFG24" alt="Amostra de uma sala gerada - Sala 976">

            
                Com o objetivo final de desenvolver uma abordagem para a estimativa cega de parâmetros acústicos, este estágio inicial, de natureza experimental e exploratória, consistiu em implementar um modelo RESNET-50 para estimar as seguintes variáveis:
            </p>
            <ul>
               <li>Área Superficial (m²)</li>
               <li>Volume (m³)</li>
               <li>Superfície 1 (m²)</li>
               <li>Superfície 2 (m²)</li>
               <li>Superfície 3 (m²)</li>
               <li>Superfície 4 (m²)</li>
               <li>Superfície 5 (m²)</li>
               <li>Superfície 6 (m²)</li>
               <li>Absorção Média (125 Hz)</li>
               <li>Absorção Média (250 Hz)</li>
               <li>Absorção Média (500 Hz)</li>
               <li>Absorção Média (1000 Hz)</li>
               <li>Absorção Média (2000 Hz)</li>
               <li>Absorção Média (4000 Hz)</li>
               <li>T60_Sabine_125</li>
               <li>T60_Sabine_250</li>
               <li>T60_Sabine_500</li>
               <li>T60_Sabine_1000</li>
               <li>T60_Sabine_2000</li>
               <li>T60_Sabine_4000</li>
            </ul>
            <p>O resultado preliminar encontra-se no seguinte link: <a href="link-resultados" target="_blank">Resultados</a>.</p>
            <p>O modelo treinado encontra-se no seguinte link: <a href="link-modelo" target="_blank">Modelo Treinado</a>.</p>
            
            <h4>Características do Modelo</h4>
            <ul>
                <li>Base: ResNet-50 pré-treinada (ImageNet).</li>
                <li>Camadas finais removidas (mantendo apenas a extração de características).</li>
                <li>Camadas adicionais: 
                    <ul>
                        <li>Global Average Pooling (reduzindo a saída para 2048 neurônios).</li>
                        <li>Uma cabeça para predição dos valores esperados (mu_head).</li>
                        <li>Uma cabeça para predição da incerteza (sigma_head), ativada com Softplus.</li>
                    </ul>
                </li>
            </ul>

            <h4>Função de Perda - Log-Verossimilhança Gaussiana</h4>
            <p>
                O modelo utiliza uma função de perda baseada em distribuição Gaussiana para estimar a incerteza da predição. A equação utilizada é:
            </p>
            <pre>
            L = 0.5 * Σ (log(σ²) + ((y - μ)² / σ²))
            </pre>
            <p>Essa abordagem permite que o modelo aprenda não apenas os valores das variáveis acústicas, mas também o nível de incerteza associado às predições.</p>

            <h4>Hiperparâmetros do Treinamento</h4>
            <ul>
                <li>Épocas: 50</li>
                <li>Otimizador: Adam (lr=1e-5)</li>
                <li>Divisão dos Dados: 80% Treino, 20% Validação</li>
                <li>Checkpoint: Salvo a cada época</li>
                <li>GPU utilizada: NVIDIA T4</li>
            </ul>

            <h4>Treinamento e Validação</h4>
            <p>
                Durante o treinamento, o modelo foi avaliado em cada época utilizando a função de perda de log-verossimilhança Gaussiana. O progresso do treinamento e validação foi registrado, gerando um gráfico de perdas ao longo das épocas.
            </p>
            <ul>
                <li>Acurácia no treino: X%</li>
                <li>Erro de treino: Y</li>
                <li>Erro de validação: Z</li>
                <li>Acurácia no teste: W%</li>
                <li>Erro no teste: V</li>
            </ul>

            <h4>Visualização das Perdas</h4>
            <p>O gráfico abaixo mostra a evolução da perda durante o treinamento:</p>
            <img src="link-grafico" alt="Gráfico de Treinamento e Validação" style="width: 100%; max-width: 600px;">
        </div>

        <!-- Espaço reservado para Projeto 1 -->
        <div class="projeto">
            <h3><a href="link-projeto-2" target="_blank">Projeto 1</a></h3>
            <p>
                ALTERAÇÕES IMPLEMENTADAS:
                
                A ETAPA I: CONFIGURAÇÃO DOS AMBIENTES; ETAPA II: IMPLEMENTAÇÃO E TREINAMENTO DOS MODELOS; ETAPA III: TESTE DOS MODELOS; ETAPA IV: AVALIAÇÃO DAS MÉTRICAS DOS MODELOS.
            <ul>
               <li>ETAPA I: CONFIGURAÇÃO DOS AMBIENTES</li>
            </ul>  
            
            <ul>
               <li>PASSO 1: GERANDO AS CONFIGURAÇÕES DA SALA</li>
            </ul>   
                Inicialmente, as dimensões das salas foram definidas aleatoriamente dentro de um intervalo preestabelecido. O comprimento e a largura variaram entre 3 e 12 metros, enquanto a altura assumiu valores entre 2,5 e 4,5 metros. Esses valores foram arredondados para uma casa decimal para garantir maior precisão nas simulações.
                Além das dimensões, foram atribuídos coeficientes de difusão para cada superfície da sala. O mesmo valor foi aplicado uniformemente a todas as superfícies e foi sorteado dentro de um intervalo entre 0,2 e 1. Esse coeficiente foi gerado para seis superfícies distintas: quatro paredes, teto e chão. Os valores foram armazenados de acordo com a frequência correspondente, considerando seis faixas padronizadas: 125 Hz, 250 Hz, 500 Hz, 1000 Hz, 2000 Hz e 4000 Hz.
                Para determinar os coeficientes de absorção, com o fim de definir perfis distintos para paredes, teto e chão, foi utilizado um processo probabilístico apresentado em  <a href="https://drive.google.com/file/d/11VNCIks217v6AZXI_0JtuiTwnnlTSDv-/view?usp=drive_link" download><button>Mean Absorption Estimation from Room Impulse Responses using Virtually-Supervised Learning</button></a></p> Em alguns casos, um único valor foi atribuído a todas as superfícies de um mesmo tipo, enquanto em outros, valores diferenciados foram gerados para cada frequência. Esses coeficientes foram estabelecidos a partir de intervalos que variaram conforme o tipo de superfície, refletindo as propriedades acústicas de materiais comuns.
                As paredes puderam apresentar coeficientes de absorção homogêneos ou diferenciados por frequência, variando entre valores baixos, típicos de materiais reflexivos, e valores mais elevados, condizentes com superfícies mais absorventes. O teto e o chão seguiram uma lógica semelhante, com intervalos de absorção distintos de acordo com materiais típicos desses elementos estruturais.
                Dessa forma, o código gerou automaticamente um conjunto de parâmetros realistas para diferentes configurações de salas, permitindo a simulação de ambientes acústicos diversos com base nas propriedades físicas de suas superfícies.

             <ul>
               <li>PASSO 2: GERANDO AS CONFIGURAÇÕES DOS RECEPTORES </li>
            </ul>

    </section>

    


    <section id="api-estimacao">
        <h2>API para Estimação</h2>
             <p>Esta API fornece funcionalidades avançadas para estimar parâmetros acústicos e realizar processamento de sinais.</p>
             <p>Acesse a API clicando no link abaixo:</p>
             <a href="https://seu-link-da-api.com" target="_blank">Acesse a API para Estimação</a>
    </section>


    

  
    
    <section id="artigos">
        <h2>Artigos Publicados</h2>
        <div class="artigo">
            <h3>Unreal Engine 5 Simulations of Solar Plant Inspections by Unmanned Aerial Systems with Robot Operating System 2</h3>
            <p><strong>Autores:</strong> Fabio Andrade, Agnar Sivertsen, Marcos G L Moura, Lucas Cavalcante Clarino, Gabriel Souza Machado Gonzaléz, Luis Paulo Albuquerque Guedes, Carlos Alberto Correia, Mariane Petraglia, Alessandro Rosa Lopes Zachi </p>
            <p><strong>Resumo:</strong> Este artigo apresenta um sistema de simulação de alta fidelidade para inspeção de usinas solares utilizando Sistemas Aéreos Não Tripulados. Ele integra o Unreal Engine 5 para visuais realistas, o AirSim para simulação da física e sensores de drones e o Robot Operating System 2 para desenvolvimento de algoritmos. O sistema permite testar algoritmos de inspeção em um ambiente virtual realista, aprimorando o desenvolvimento de soluções de visão computacional e detecção de objetos. Um estudo de caso utilizando a Unidade de Controle de Voo ArduPilot, Detecção de Bordas de Canny e controle PID demonstra sua eficácia.</p>
            <p><a href="link-artigo" target="_blank">Acesse o artigo completo</a></p>
        </div>
        
        <div class="artigo">
            <h3>Radio Frequency-Audio Based Drone Classification using Deep Learning Methods</h3>
            <p><strong>Autores:</strong> Luis Paulo Albuquerque Guedes, Mariane Petraglia, Rêmulo M. Caminha Gomes</p>
            <p><strong>Resumo:</strong> Este artigo propõe um modelo de aprendizado profundo que utiliza a fusão de dados de sensores acústicos e de radiofrequência (RF) para a classificação de Veículos Aéreos Não Tripulados (UAVs). O modelo visa aumentar a precisão na classificação de diferentes tipos de drones, garantindo redundância para evitar falhas. O trabalho foi estruturado em três etapas: análise acústica para classificação de tipos de drones (etapa 1), classificação baseada em assinaturas de RF (etapa 2) e integração dos sensores (etapa 3). A combinação de sensores acústicos e de RF resultou em uma melhoria significativa no desempenho da classificação de drones, aumentando a precisão de 0.8342 para 0.8617. Além disso, o uso dessa fusão de dados melhorou a precisão em até 15,3% em faixas de SNR mais baixas.</p>
            <p><a href="link-artigo" target="_blank">Acesse o artigo completo</a></p>
        </div>
        
        <div class="artigo">
            <h3>An Integrated Framework for UAV Sound Tracking and Classification Using Deep Learning Techniques and Kalman Filters</h3>
            <p><strong>Autores:</strong> Luis Paulo Albuquerque Guedes, Mariane Petraglia, Pedro Henrique Monteiro Guedes</p>
            <p><strong>Resumo:</strong> Este artigo apresenta um framework integrado que combina classificação acústica e rastreamento de trajetória de Veículos Aéreos Não Tripulados (UAVs) utilizando técnicas de aprendizado profundo e filtros de Kalman. A abordagem proposta visa melhorar a precisão na identificação do tipo de UAV e na predição de trajetória, com base em dados acústicos passivos reais registrados em uma câmara anecoica e convolvidos com ruído ambiental. Os resultados das simulações confirmaram a eficácia do sistema tanto nas tarefas de classificação quanto na estimativa de trajetória. Para a classificação do tipo de UAV, o F1-score geral atingiu 0.8342, com o Drone C obtendo o melhor desempenho (0.8504). Para a classificação da direção de manobra, os Drones A e B alcançaram um F1-score de 0.8277, enquanto o Drone C obteve 0.8298. O erro médio de distância na estimativa de trajetória foi de aproximadamente 8 metros, com desvios padrão variando de 3.34 a 3.55 metros, demonstrando consistência e precisão em todas as dimensões avaliadas.</p>
            <p><a href="link-artigo" target="_blank">Acesse o artigo completo</a></p>
        </div>

     </section>

     <section id="livros">
        <h2>Livros Recomendados</h2>
        <p>Aqui estão alguns livros recomendados sobre modelagem acústica e processamento de sinais:</p>
        <ul>
            <li><a href="link-livro-1" target="_blank">Livro 1</a></li>
            <li><a href="link-livro-2" target="_blank">Livro 2</a></li>
            <li><a href="link-livro-3" target="_blank">Livro 3</a></li>
        <!-- Adicione mais livros conforme necessário -->
        </ul>
     </section>
    

     <section id="contato">
        <h2>Contato</h2>
        <ul>
            <li>
                <img src="email-icon.png" alt="Email" width="20"> 
                <a href="mailto:luis.albuquerque@marinha.mil.br">luis.albuquerque@marinha.mil.br</a>
            </li>
            <li>
                <img src="email-icon.png" alt="Email" width="20"> 
                <a href="mailto:luis.guedes@coppe.ufrj.br">luis.guedes@coppe.ufrj.br</a>
            </li>
            <li>
                <img src="email-icon.png" alt="Email" width="20"> 
                <a href="mailto:luispauloaguedes@gmail.com">luispauloaguedes@gmail.com</a>
            </li>
            <li>
                <img src="github-icon.png" alt="GitHub" width="20"> 
                <a href="https://github.com/seuusuario" target="_blank">GitHub</a>
            </li>
            <li>
                <img src="linkedin-icon.png" alt="LinkedIn" width="20"> 
                <a href="https://www.linkedin.com/in/seuperfil" target="_blank">LinkedIn</a>
            </li>
        </ul>
    </section>

    <script>
        function translate(language) {
            if (language === 'en') {
                window.location.href = "pagina-ingles.html";
            } else if (language === 'de') {
                window.location.href = "pagina-alemao.html";
            }
        }
    </script>

    <script>
        function translate(language) {
            if (language === 'en') {
                window.location.href = "pagina-ingles.html";  // Página em inglês
            } else if (language === 'de') {
                window.location.href = "pagina-alemao.html";  // Página em alemão
            }
        }
    </script>

</body>
</html>
