<!DOCTYPE html>
            <html lang="pt-BR">
            <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Modelagem Ac√∫stica e Estima√ß√£o de Par√¢metros</title>
            <link rel="stylesheet" href="style.css">
            </head>
            <body>


            <header>
            <h1>Modelagem Ac√∫stica, Processamento de Sinais de √Åudio e Processamento de Imagens</h1>
            <nav>
            <div class="language-switch">
            <a href="index-en.html">üá¨üáß EN</a>
            <a href="index-de.html">üá©üá™ DE</a>
            </div>
            <ul>
            <li><a href="#sobre">Sobre</a></li>
            <li><a href="#dissertacao">Disserta√ß√£o</a></li>
            <li><a href="#bibliografia">Bibliografia</a></li> <!-- Link para a se√ß√£o Bibliografia -->
            <li><a href="#projetos">Projetos</a></li>
            <li><a href="#api-estimacao">API para estima√ß√£o</a></li> <!-- Nova aba adicionada -->
            <li><a href="#artigos">Artigos publicados</a></li>
            <li><a href="#livros">Livros recomendados</a></li> <!-- Link para a se√ß√£o Livros Recomendados -->
            <li><a href="#contato">Contato</a></li>
            </ul>
            </nav>
            </header>

            <section id="sobre">
            <h2>Sobre</h2>
            <p>Ol√°! Meu nome √© Luis Paulo Albuquerque Guedes. Sou aluno de mestrado em Engenharia El√©trica, na √°rea de Processamento de Sinais de √Åudio, desenvolvida na COPPE/UFRJ, sob orienta√ß√£o da Professora Mariane Petraglia (<a href="http://lattes.cnpq.br/7226228571158486" download><button>Lattes</button></a>)  e do Professor Julio Cesar Boscher Torres (<a href="http://lattes.cnpq.br/3706003646448210" download><button>Lattes</button></a>).   
               Inicialmente, este reposit√≥rio tem como objetivo reunir os registros de progresso da minha disserta√ß√£o de mestrado.</p>

            <p>Meu curriculum vitae encontra-se dispon√≠vel em: <a href="https://drive.google.com/uc?export=download&id=1bxuK9c2iCFkGVn5Ir6R0GLqEurPVsBQe" download><button>Baixar CV</button></a>.</p>
            </section>

            <section id="dissertacao">
            <h2>Disserta√ß√£o</h2>
            <p>Confira os detalhes da vers√£o final da disserta√ß√£o: <a href="link-da-dissertacao" target="_blank">Acesse aqui</a>.</p>
            </section>

            <section id="bibliografia">
            <h2>Bibliografia</h2>
            <p>Confira a bibliografia utilizada para a disserta√ß√£o:</p>
            <ul>
            <li><a href="link-bibliografia-1" target="_blank">Refer√™ncia Bibliogr√°fica 1</a></li>
            <li><a href="link-bibliografia-2" target="_blank">Refer√™ncia Bibliogr√°fica 2</a></li>
            <li><a href="link-bibliografia-3" target="_blank">Refer√™ncia Bibliogr√°fica 3</a></li>
        <!-- Adicione mais refer√™ncias conforme necess√°rio -->
            </ul>
            </section>

            <style>
            ul {
            padding-left: 0;
            list-style-position: inside;
            }
            </style>

            <section id="projetos">
            <h2>Meus Projetos</h2>
            <p>Trata-se este reposit√≥rio dos registros do progresso das simula√ß√µes ac√∫sticas da disserta√ß√£o de mestrado sobre estima√ß√£o de par√¢metros ac√∫sticos usando modelos de Aprendizagem Profunda (Deep Learning).  </p>
        
          <!-- Espa√ßo reservado para Projeto 3 -->
            <div class="projeto">
            <h3><a href="link-projeto-3" target="_blank">Projeto 3</a></h3>
            <p>
              ALTERA√á√ïES 
            </p>
            <ul>
            <img src="fixed_areas.png" alt="3D Room" class="responsive-image">
            <figcaption>Visualiza√ß√£o 3D de uma amostra de sala simulada com regi√µes transl√∫cidas que representam o volume em que as posi√ß√µes das fontes de ru√≠dos podem variar</figcaption>
            </ul>
            
            </p>
            <p>Mais informa√ß√µes podem ser encontradas no seguinte link: <a href="link-resultados-3" target="_blank">Resultados do Projeto 3</a>.</p>
            </div>


        <!-- Projeto 2 -->
            <div class="projeto">
            <h3><a href="link-projeto-2" target="_blank">Projeto 2</a></h3>
            <p>
                ALTERA√á√ïES IMPLEMENTADAS:
                
                A ETAPA I: CONFIGURA√á√ÉO DOS AMBIENTES; ETAPA II: GERA√á√ÉO DE RESPOSTAS AO IMPULSO DE SALAS (RIS); ETAPA III: IMPLEMENTA√á√ÉO E TREINAMENTO DOS MODELOS; ETAPA III: TESTE DOS MODELOS; ETAPA IV: AVALIA√á√ÉO DAS M√âTRICAS DOS MODELOS.
            <ul>
            <li>ETAPA I: CONFIGURA√á√ÉO DOS AMBIENTES</li>
            </ul>  
            
            <ul>
            <li>PASSO 1: GERANDO AS CONFIGURA√á√ïES DA SALA</li>
            </ul>   
                Inicialmente, as dimens√µes das salas foram definidas aleatoriamente dentro de um intervalo preestabelecido. O comprimento e a largura variaram entre 3 e 12 metros, enquanto a altura assumiu valores entre 2,5 e 4,5 metros. Esses valores foram arredondados para uma casa decimal para garantir maior precis√£o nas simula√ß√µes.
                Al√©m das dimens√µes, foram atribu√≠dos coeficientes de difus√£o para cada superf√≠cie da sala. O mesmo valor foi aplicado uniformemente a todas as superf√≠cies e foi sorteado dentro de um intervalo entre 0,2 e 1. Esse coeficiente foi gerado para seis superf√≠cies distintas: quatro paredes, teto e ch√£o. Os valores foram armazenados de acordo com a frequ√™ncia correspondente, considerando seis faixas padronizadas: 125 Hz, 250 Hz, 500 Hz, 1000 Hz, 2000 Hz e 4000 Hz.
                Para determinar os coeficientes de absor√ß√£o, com o fim de definir perfis distintos para paredes, teto e ch√£o, foi utilizado um processo probabil√≠stico apresentado em:  <a href="https://drive.google.com/file/d/11VNCIks217v6AZXI_0JtuiTwnnlTSDv-/view?usp=drive_link" download><button>Mean Absorption Estimation from Room Impulse Responses using Virtually-Supervised Learning</button></a>.</p> Em alguns casos, um √∫nico valor foi atribu√≠do a todas as superf√≠cies de um mesmo tipo, enquanto em outros, valores diferenciados foram gerados para cada frequ√™ncia. Esses coeficientes foram estabelecidos a partir de intervalos que variaram conforme o tipo de superf√≠cie, refletindo as propriedades ac√∫sticas de materiais comuns.
                As paredes puderam apresentar coeficientes de absor√ß√£o homog√™neos ou diferenciados por frequ√™ncia, variando entre valores baixos, t√≠picos de materiais reflexivos, e valores mais elevados, condizentes com superf√≠cies mais absorventes. O teto e o ch√£o seguiram uma l√≥gica semelhante, com intervalos de absor√ß√£o distintos de acordo com materiais t√≠picos desses elementos estruturais.
                Dessa forma, o c√≥digo gerou automaticamente um conjunto de par√¢metros realistas para diferentes configura√ß√µes de salas, permitindo a simula√ß√£o de ambientes ac√∫sticos diversos com base nas propriedades f√≠sicas de suas superf√≠cies.
             
            <ul>
            <img src="room_3D.png" alt="3D Room" class="responsive-image">
            <figcaption>Visualiza√ß√£o 3D de uma amostra de sala simulada</figcaption>
            </ul>
             
            
            <ul>
            <li>PASSO 2: GERANDO AS CONFIGURA√á√ïES DOS RECEPTORES </li>
            </ul>
               O c√≥digo define uma classe chamada 'util_receiver', respons√°vel por gerar e visualizar a disposi√ß√£o de receptores e microfones em um ambiente tridimensional. Primeiramente, s√£o inicializados atributos para armazenar as informa√ß√µes dos receptores. Em seguida, para cada receptor, √© gerado um ponto central (baricentro) dentro de um espa√ßo tridimensional, 
               garantindo que esteja dentro dos limites definidos pela sala. A partir desse baricentro, s√£o calculadas as posi√ß√µes de dois microfones simetricamente distribu√≠dos ao longo do eixo X, garantindo um espa√ßamento fixo entre eles. Um vetor de dire√ß√£o perpendicular ao eixo que une os microfones √© ent√£o gerado, permitindo que a orienta√ß√£o do receptor varie 
               dentro do plano XY. Ap√≥s a gera√ß√£o dos receptores, microfones e vetores direcionais, um gr√°fico tridimensional interativo √© constru√≠do utilizando a biblioteca Plotly. O layout do gr√°fico √© configurado para exibir os eixos e uma legenda, proporcionando uma visualiza√ß√£o clara da distribui√ß√£o dos elementos no espa√ßo tridimensional.

            <ul>
            <li>PASSO 3: GERANDO AS CONFIGURA√á√ïES DAS FONTES DE FALA </li>
            </ul>
               O c√≥digo define uma classe chamada 'util_source', respons√°vel por gerar e posicionar fontes de som dentro de um ambiente tridimensional. Inicialmente, s√£o listados diferentes tipos de fontes sonoras, como omnidirecional, cardioide e bidirecional. Para criar fontes reais na sala, s√£o geradas coordenadas aleat√≥rias garantindo uma dist√¢ncia m√≠nima segura em rela√ß√£o a um ponto de refer√™ncia (baricentro). 
               A primeira fonte sempre ser√° omnidirecional, enquanto as demais s√£o atribu√≠das aleatoriamente a uma das categorias dispon√≠veis. Al√©m disso, cada fonte recebe um vetor de dire√ß√£o, que pode ser perpendicular ao plano horizontal ou gerado aleatoriamente.
               No caso das fontes de ru√≠do, s√£o definidos at√© cinco pontos estrat√©gicos, incluindo posi√ß√µes fixas no centro superior da sala e ao longo das extremidades, enquanto fontes adicionais s√£o posicionadas aleatoriamente dentro dos limites da sala. As dire√ß√µes dessas fontes de ru√≠do podem ser fixas (por exemplo, apontando para baixo no eixo Z) ou geradas aleatoriamente. Cada fonte recebe um √¢ngulo de orienta√ß√£o (yaw) aleat√≥rio dentro de um intervalo de -180 a 180 graus para garantir diversidade na orienta√ß√£o espacial. 
               Para evitar sobreposi√ß√µes ou posicionamentos inadequados, s√£o utilizadas verifica√ß√µes de dist√¢ncia m√≠nima entre as fontes e seus respectivos baricentros. Al√©m disso, vetores perpendiculares s√£o calculados para manter coer√™ncia na orienta√ß√£o das fontes, garantindo que suas dire√ß√µes estejam bem distribu√≠das dentro do espa√ßo tridimensional da sala.

         
            <ul>
            <li>PASSO 4: GERANDO AS CONFIGURA√á√ïES DAS FONTES DE RU√çDO </li>
            </ul>

              O c√≥digo gera e visualiza fontes de ru√≠do dentro de uma sala tridimensional simulada, associando cada fonte a um arquivo de som espec√≠fico e a uma √°rea de atua√ß√£o realista. A classe util_source posiciona estrategicamente os ru√≠dos no ambiente, garantindo que sua distribui√ß√£o reflita situa√ß√µes reais. Cada ru√≠do possui um intervalo de posicionamento definido no c√≥digo, assegurando coer√™ncia espacial com sua fonte original.
              Os ru√≠dos utilizados incluem ar-condicionado, tosse, risada, telefone tocando e digita√ß√£o no teclado, extra√≠dos do conjunto DCASE 2016 Task 2, dispon√≠veis em: <a href="https://dcase.community/challenge2016/task-sound-event-detection-in-synthetic-audio" download><button>Sound event detection in synthetic audio</button></a>.</p>  O ar-condicionado √© posicionado no teto, simulando um sistema de ventila√ß√£o. A tosse ocorre em altura m√©dia, representando uma pessoa no ambiente. 
              A risada √© colocada pr√≥xima √†s laterais, indicando conversas em grupo. O telefone aparece pr√≥ximo √†s extremidades da sala, em uma mesa ou esta√ß√£o de trabalho. J√° a digita√ß√£o √© posicionada pr√≥xima ao ch√£o, simulando teclados sobre superf√≠cies.
              A biblioteca Plotly √© usada para gerar um gr√°fico 3D interativo, onde cada fonte de ru√≠do √© representada por um marcador vermelho e vetores direcionais azuis indicam a propaga√ß√£o do som. Al√©m disso, as informa√ß√µes sobre posi√ß√£o, dire√ß√£o e arquivo de som correspondente s√£o organizadas em uma tabela formatada com a biblioteca tabulate.
              Essa abordagem garante que a simula√ß√£o seja realista e contextualizada, permitindo estudos avan√ßados sobre propaga√ß√£o ac√∫stica, interfer√™ncia sonora e inteligibilidade da fala em ambientes fechados. 
            
            <ul>
            <img src="room_sources.png" alt="3D Room" class="responsive-image">
            <figcaption>Visualiza√ß√£o 3D de uma amostra de sala simulada com microfones, fontes de fala e de ru√≠do</figcaption>
            </ul>
            
            <ul>
            <li>PASSO 5: COMPILANDO AS INFORMA√á√ïES</li>
            </ul>
               O c√≥digo define uma classe chamada conf_files, respons√°vel por gerar configura√ß√µes de salas para simula√ß√µes ac√∫sticas. Ao ser inicializada, a classe recebe o n√∫mero de salas a serem geradas e o nome do conjunto de dados. Em seguida, s√£o instanciadas as classes auxiliares respons√°veis pela defini√ß√£o dos receptores, fontes sonoras e propriedades da sala. Um diret√≥rio de armazenamento √© criado para salvar os arquivos de configura√ß√£o.
               Inicialmente, s√£o gerados os par√¢metros globais da simula√ß√£o, incluindo a taxa de amostragem, a dura√ß√£o da resposta ao impulso, a ordem de reflex√£o e a quantidade de raios para o tra√ßado estoc√°stico. Esses par√¢metros s√£o armazenados em arquivos nos formatos YAML e JSON. Na etapa de cria√ß√£o das salas, um dicion√°rio √© estruturado para armazenar informa√ß√µes sobre as dimens√µes da sala, coeficientes de absor√ß√£o e difus√£o, al√©m das condi√ß√µes ambientais, 
               como umidade e temperatura. Para cada sala, s√£o geradas posi√ß√µes de receptores e suas respectivas dire√ß√µes, assegurando um espa√ßamento adequado. Fontes sonoras tamb√©m s√£o distribu√≠das dentro do ambiente, garantindo que pelo menos uma delas seja omnidirecional. Al√©m disso, fontes de ru√≠do s√£o posicionadas em locais espec√≠ficos, como pr√≥ximos ao teto, lateralmente e perto do ch√£o, simulando diferentes condi√ß√µes ac√∫sticas.
               Ao final do processo, os dados estruturados s√£o armazenados em arquivos YAML e JSON, organizando as informa√ß√µes de cada sala, incluindo os par√¢metros de simula√ß√£o, receptores, fontes sonoras e fontes de ru√≠do. O progresso da gera√ß√£o das salas √© monitorado com uma barra de progresso para indicar a conclus√£o das itera√ß√µes.
            
            <ul>
            <li>PASSO 6: GERANDO E ARMAZENADOS AS SALAS</li>
            </ul>
                O c√≥digo cria 1000 salas e salva suas configura√ß√µes nos formatos YAML e JSON. Para cada sala, s√£o geradas dimens√µes, receptores, fontes sonoras (incluindo uma omnidirecional) e fontes de ru√≠do posicionadas estrategicamente. Os dados s√£o armazenados em arquivos organizados, e o progresso √© exibido com tqdm.
             
            <ul>
            <li>ETAPA II: GERA√á√ÉO DE RESPOSTAS AO IMPULSO DE SALAS (RIS)</li>
            </ul>  

                A segunda etapa teve como objetivo a implementa√ß√£o da biblioteca gpuRIR, uma ferramenta de c√≥digo aberto em Python para a simula√ß√£o de Respostas ao Impulso de Salas (RIRs). Essa biblioteca utiliza o Image Source Method (ISM) com acelera√ß√£o via GPU, 
                permitindo o c√°lculo eficiente de RIRs entre m√∫ltiplas fontes e receptores em paralelo por meio de GPUs CUDA. Comparada √†s implementa√ß√µes tradicionais baseadas em CPU, a gpuRIR oferece um desempenho significativamente superior, sendo aproximadamente 100 vezes mais r√°pida. A biblioteca gpuRIR est√° dispon√≠vel em: <a href="https://github.com/DavidDiazGuerra/gpuRIR" download><button>gpuRIR</button></a>.
                
            <ul>
                O artigo que aborda a biblioteca gpuRIR pode ser encontrado em:  <a href="https://link.springer.com/article/10.1007/s11042-020-09905-3" download><button>gpuRIR: A python library for room impulse response simulation with GPU acceleration</button></a>.
            </ul> 
            <ul>
            <img src="gpuRIR.png" alt="3D Room" class="responsive-image">
            <figcaption>Biblioteca gpuRIR</figcaption>
            </ul>

            <ul>
                A biblioteca gpuRIR gera RIRs para diferentes configura√ß√µes ac√∫sticas utilizando a biblioteca gpuRIR. Primeiramente, ele carrega os dados das salas, fontes sonoras e receptores a partir de arquivos Parquet. Com base nas dimens√µes da sala, coeficientes de absor√ß√£o e par√¢metros ac√∫sticos como T60, C50 e DRR, o c√≥digo calcula os coeficientes de reflex√£o das superf√≠cies e determina a ordem de imagem necess√°ria para a simula√ß√£o. Em seguida, para cada frequ√™ncia especificada (125, 250, 500, 1000, 2000 e 4000 Hz), s√£o geradas RIRs para todas as fontes sonoras, 
                levando em considera√ß√£o o padr√£o direcional das fontes e microfones. As RIRs s√£o simuladas separadamente para dois conjuntos de microfones e, ao final do processamento, os resultados s√£o armazenados em arquivos Parquet organizados por frequ√™ncia. O c√≥digo tamb√©m exibe o n√∫mero de fontes e receptores presentes na sala selecionada antes de finalizar a execu√ß√£o.
            </ul>

            <ul>
            <img src="RIR_sources.png" alt="3D Room" class="responsive-image">
            <figcaption>RIRs geradas para fontes de fala de room_0</figcaption>
            </ul>

            <ul>
            <img src="RIR_noises.png" alt="3D Room" class="responsive-image">
            <figcaption>RIRs geradas para fontes de ru√≠do de room_0</figcaption>
            </ul>

            <ul>
            <li>ETAPA III: FILTRAGEM DAS RESPOSTAS AO IMPULSO DE SALAS GERADAS </li>
            </ul>  
            <ul>
            Segundo o desenvolevor da biblioteca de gera√ß√£o de respostas ao impulso, o gpuRIR n√£o suporta absor√ß√£o dependente de frequ√™ncia (resposta do desenvolvedor sobre o assunto no f√≥rum de d√∫vidas do GitHub da biblioteca, dispon√≠vel em: <a href="https://github.com/DavidDiazGuerra/gpuRIR/issues/70#issuecomment-2541267155" download><button> F√≥rum de d√∫vidas da biblioteca gpuRIR</button></a>. 
            </ul> 
            <ul>
            Dessa maneira, foi adotada a seguinte estrat√©gia:
            </ul> 
            <ul>
            Inicialmente, foi implementado um c√≥digo que carrega os arquivos contendo os sinais de RIR extraindo as informa√ß√µes relevantes, como os √≠ndices das fontes e receptores. Em seguida, foi implementada a biblioteca PyOctaveBand (dispon√≠vel em: <a href="https://github.com/jmrplens/PyOctaveBand" download><button>Biblioteca PyOctaveBand</button></a>) para aplicar um filtro de oitava em cada RIR, 
            considerando a frequ√™ncia correspondente a cada arquivo. Ap√≥s a filtragem, as respostas ao impulso foram somadas para cada combina√ß√£o de fonte e receptor. O c√≥digo ent√£o normaliza os sinais processados e os converte para o formato de 16 bits antes de armazen√°-los como arquivos .wav.
            </ul> 

            <ul>
            <img src="pyoctaveband.png" alt="3D Room" class="responsive-image">
            <figcaption>Biblioteca PyOctaveBand</figcaption>
            </ul>
      
            <ul>
            <img src="SPL.png" alt="3D Room" class="responsive-image">
            <figcaption>Gr√°fico SPL por frequ√™ncia</figcaption>
            </ul>
            
            <ul>
            Ap√≥s a filtragem e a soma das contribui√ß√µes de cada banda, as RIRs apresentaram o seguinte padr√£o:
            </ul>
            <ul>
            <img src="RIR_filtered.png" alt="3D Room" class="responsive-image">
            <figcaption>Amostra de RIRs filtradas e somadas</figcaption>
            </ul>

            <ul>
            <li>ETAPA IV: CONVOLU√á√ÉO DE RIS COM √ÅUDIOS ANEC√ìICOS E MISTURA</li>
            </ul>  

            <ul>
            <b>1. Configura√ß√µes Iniciais</b>
            </ul>
            
            <ul>
            <li>Define as taxas de amostragem para diferentes tipos de √°udio: RIRs (48 kHz), fala (16 kHz) e ru√≠dos (44.1 kHz).</li>
            <li>Estabelece uma dura√ß√£o de √°udio de 5 segundos.</li>
            <li>Define os ganhos para diferentes fontes de ru√≠do a fim de controlar sua intensidade.</li>
            <li>Especifica os caminhos para os arquivos de fala, respostas ao impulso e ru√≠dos.</li>
            </ul>
    
            <ul>
            <b>2. Carregamento de √Åudio</b>
            </ul>
            
            <ul>
            <li>A fun√ß√£o <code>load_audio()</code> carrega arquivos de √°udio de fala e ru√≠do, realizando a reamostragem quando necess√°rio.</li>
            <li>A fun√ß√£o <code>load_rir()</code> carrega e reamostra as respostas ao impulso.</li>
            </ul>
    
            <ul>
            <b>3. Processamento de Convolu√ß√£o</b>
            </ul>
            
            <ul>
            <li>A fun√ß√£o <code>convolve_audio_with_rir()</code> realiza a convolu√ß√£o do √°udio de entrada com a RIR correspondente.</li>
            <li>A fun√ß√£o <code>pad_or_trim_audio()</code> ajusta a dura√ß√£o do √°udio convolu√≠do para 5 segundos.</li>
            <li>A fun√ß√£o <code>normalize_audio()</code> normaliza os sinais.</li>
            </ul>
    
            <ul>
            <b>4. Sele√ß√£o e Convolu√ß√£o do Sinal de Fala</b>
            </ul>
            
            <ul>
            <li>O c√≥digo identifica as fontes sonoras e os receptores dispon√≠veis.</li>
            <li>O usu√°rio escolhe uma fonte e um receptor para processar os sinais.</li>
            <li>Um arquivo de fala √© selecionado aleatoriamente e convolu√≠do com as RIRs dos dois microfones.</li>
            </ul>
    
            <ul>
            <b>5. Adi√ß√£o de Ru√≠do</b>
            </ul>
            
            <ul>
            <li>Um SNR aleat√≥rio entre 5 e 60 dB √© sorteado.</li>
            <li>Os arquivos de ru√≠do s√£o carregados e convolu√≠dos com as RIRs correspondentes.</li>
            <li>O ru√≠do √© escalado para atingir o SNR desejado antes de ser somado ao sinal de fala.</li>
            </ul>
    
            <ul>
            <b>6. Gera√ß√£o e Armazenamento do √Åudio Processado</b>
            </ul>
            
            <ul>
            <li>Os sinais misturados s√£o normalizados.</li>
            <li>Os arquivos resultantes s√£o salvos em formato .wav nos diret√≥rios especificados.</li>
            </ul>
    
            <ul>
            <b>7. C√°lculo e Exibi√ß√£o de M√©tricas</b>
            </ul>
            
            <ul>
            <li>O c√≥digo calcula o SNR real dos sinais e os exibe em uma tabela.</li>
            <li>S√£o geradas visualiza√ß√µes da forma de onda e do espectrograma.</li>
            <li>Os sinais s√£o reproduzidos para avalia√ß√£o auditiva.</li>
            </ul>

            <audio controls>
            <source src="audio_mic_1.wav" type="audio/mpeg">
            Amostra de √°udio para mic_1
            </audio>

            <audio controls>
            <source src="audio_mic_2.wav" type="audio/mpeg">
            Amostra de √°udio para mic_2
            </audio>



            <ul>
            <img src="wave_form.png" alt="3D Room" class="responsive-image">
            <figcaption>Amostras de formas de onda de RIR convolu√≠das com √°udio anec√≥ico </figcaption>
            </ul>

            <ul>
            <img src="spectrogram.png" alt="3D Room" class="responsive-image">
            <figcaption>Amostra de espectrogramas gerados a partir de √°udio</figcaption>
            </ul>

            <ul>
            <b>Identifica√ß√£o temporal dos ru√≠dos gerados: </b> O per√≠odo de dura√ß√£o dos ru√≠dos foi gerado aleatoriamente, ou seja, cada ru√≠do teve um tempo de in√≠cio e t√©rmino espec√≠fico e aleat√≥rio. A √∫nica exce√ß√£o foi o ru√≠do 1 ("airconditioning.wav"), 
            que foi configurado para durar durante todo o √°udio. 
            </ul>

            <audio controls>
            <source src="audio_setup.wav" type="audio/mpeg">
            Amostra de √°udio para mic_2
            </audio>


            <ul>
            <img src="setup_noise_duration.png" alt="3D Room" class="responsive-image">
            <figcaption>Esquema para identifica√ß√£o temporal dos ru√≠dos gerados</figcaption>
            </ul>


            <ul>
            <li>ETAPA V: C√ÅLCULO ANAL√çTICO DE RT60</li>
            </ul>  

            <ul>
            O c√≥digo processa par√¢metros ac√∫sticos de salas a partir de arquivos contendo informa√ß√µes sobre dimens√µes e coeficientes de absor√ß√£o sonora, calculando os tempos de reverbera√ß√£o (T60) utilizando os modelos de Sabine, Eyring e Millington, al√©m do NRC (Noise Reduction Coefficient). 
            Inicialmente, s√£o definidas fun√ß√µes matem√°ticas para calcular a absor√ß√£o m√©dia, o NRC e os tempos de reverbera√ß√£o com base em diferentes modelos, considerando a √°rea superficial, o volume e os coeficientes de absor√ß√£o das salas. A partir do arquivo absorption_results.csv, aque √© respons√°vel por armazenar a absor√ß√£o m√©dia para cada frequ√™ncia espec√≠fica (125 Hz, 250 Hz, 500 Hz, 1000 Hz, 2000 Hz e 4000 Hz), e do arquivo rooms.parquet, os tempos de reverbera√ß√£o s√£o calculados para cada frequ√™ncia com base na absor√ß√£o m√©dia registrada no arquivo de absor√ß√£o. 
            </ul>  

            <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Par√¢metros Ac√∫sticos e C√°lculo do Tempo de Reverbera√ß√£o</title>
            <script type="text/javascript" async
            src="https://polyfill.io/v3/polyfill.min.js?features=es6">
            </script>
            <script type="text/javascript" async
            id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
            </script>
            </head>
            <body>
            <h1>Par√¢metros Ac√∫sticos e C√°lculo do Tempo de Reverbera√ß√£o</h1>

            <p>
            O c√≥digo processa par√¢metros ac√∫sticos de salas a partir de arquivos contendo informa√ß√µes sobre dimens√µes e coeficientes de absor√ß√£o sonora, 
            calculando os tempos de reverbera√ß√£o (<b>T60</b>) utilizando os modelos de <b>Sabine</b>, <b>Eyring</b> e <b>Millington</b>, 
            al√©m do <b>NRC (Noise Reduction Coefficient)</b>.
            </p>
    
            <ul>
            <b>1. Absor√ß√£o M√©dia</b>
            </ul>
      
            <p>
            A absor√ß√£o m√©dia ponderada das superf√≠cies da sala √© calculada por:
            </p>
            <p>
            \[
            \alpha_{m√©dia} = \frac{\sum S_i \alpha_i}{\sum S_i}
            \]
            </p>
            <p>Onde:</p>
            <ul>
            <li>\( S_i \) = √°rea de cada superf√≠cie</li>
            <li>\( \alpha_i \) = coeficiente de absor√ß√£o da superf√≠cie</li>
            </ul>

            <ul>
            <b>2. Tempo de Reverbera√ß√£o (T60)</b>
            </ul>
            <p>
            O <b>tempo de reverbera√ß√£o</b> representa o tempo necess√°rio para que o som em um ambiente 
            <b>diminua em 60 dB</b> ap√≥s a fonte sonora ser desligada.
            </p>

            <ul> F√≥rmula de Sabine</ul>
            <p>
            \[
            T_{60} = \frac{24 \ln(10) \cdot V}{c \cdot (S \cdot \alpha)}
            \]
            </p>
            <p>Em que:</p>
            <ul>
            <li>\( V \) = volume da sala (m¬≥)</li>
            <li>\( c \) = velocidade do som (m/s)</li>
            <li>\( S \) = √°rea total das superf√≠cies (m¬≤)</li>
            <li>\( \alpha \) = coeficiente de absor√ß√£o m√©dio</li>
            </ul>

            <ul> F√≥rmula de Eyring</ul>
            <p>
            \[
            T_{60} = \frac{24 \ln(10) \cdot V}{c \cdot (-S \ln(1 - \alpha))}
             \]
            </p>

            <ul> F√≥rmula de Millington</ul>
            <p>
            \[
            T_{60} = \frac{24 \ln(10) \cdot V}{c \cdot \sum (-S_i \ln(1 - \alpha_i))}
            \]
           </p>

           </body>
           <ul> 
           Foi implementado um c√≥digo para calcular C50, C80 e T60, a partir de sinais de √°udio. A fun√ß√£o clarity(time, signal, fs, bands) calcula o √≠ndice de clareza C50 (para fala) ou C80 (para m√∫sica), filtrando o sinal de acordo com bandas de oitava e comparando a energia acumulada nos primeiros milissegundos com a energia restante. 
           As fun√ß√µes c50_from_file(file_path, bands) e c80_from_file(file_path, bands) extraem o √≠ndice de clareza diretamente de um arquivo de √°udio. J√° a fun√ß√£o t60_impulse(file_name, bands, rt='t30') estima o tempo de reverbera√ß√£o T60 com diferentes m√©todos (T30, T20, T10 e EDT), baseando-se no decaimento energ√©tico do sinal filtrado em bandas de oitava e ajustando uma regress√£o linear sobre a curva de energia cumulativa. 
           </ul>
           Tamb√©m foi extra√≠da a dist√°ncia eucliana entre cada microfone e fonte, e o DRR (Direct-to-Reverberant Ratio), conforme a seguir:

           <!DOCTYPE html>
           <html>
           <head>
           <meta charset="UTF-8">
           <meta name="viewport" content="width=device-width, initial-scale=1.0">
           <title>C√°lculo do DRR (Direct-to-Reverberant Ratio)</title>
           <script type="text/javascript" async
           src="https://polyfill.io/v3/polyfill.min.js?features=es6">
           </script>
           <script type="text/javascript" async
           id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
           </script>
           </head>
           <body>
           <h1>F√≥rmula do C√°lculo do DRR (Direct-to-Reverberant Ratio)</h1>
           <p>O DRR √© calculado utilizando a seguinte equa√ß√£o:</p>
           <p style="font-size: 15px; font-weight: bold;">
           \[ DRR = 10 \log_{10} \left( \frac{E_{direct}}{E_{reverberant}} \right) \]
           </p>
           <p>Onde:</p>
           <ul>
          
           <li><b>DRR</b>: Direct-to-Reverberant Ratio (dB)</li>
           <li><b>E<sub>direct</sub></b>: Energia da parte direta do sinal</li>
           <li><b>E<sub>reverberant</sub></b>: Energia da parte reverberante do sinal</li>

           </ul>
           </body>
           </html>

 
           Tamb√©m foi calculada a Frequ√™ncia de Schroeder (FS), que se trata de um valor que marca a transi√ß√£o entre dois regimes distintos no comportamento ac√∫stico de um ambiente fechado:
           <ul> 
           Abaixo da Frequ√™ncia de Schroeder: O campo sonoro √© dominado por modos pr√≥prios (modos ressonantes), e o som se comporta de maneira mais discreta e n√£o homog√™nea. √â comum que em baixas frequ√™ncias haja resson√¢ncias e ac√∫mulos de energia em certos pontos da sala.
           </ul>   
           <ul> 
           Acima da Frequ√™ncia de Schroeder: O campo sonoro passa a ser considerado difuso e estoc√°stico. Os modos pr√≥prios se sobrep√µem de forma densa, e a propaga√ß√£o do som se aproxima mais dos pressupostos da ac√∫stica estat√≠stica e da reverbera√ß√£o homog√™nea.
           </ul>

           <head>
           <meta charset="UTF-8">
           <meta name="viewport" content="width=device-width, initial-scale=1.0">
           <title>Par√¢metros Ac√∫sticos e C√°lculo do Tempo de Reverbera√ß√£o</title>
           <script type="text/javascript" async
           src="https://polyfill.io/v3/polyfill.min.js?features=es6">
           </script>
           <script type="text/javascript" async
           id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
           </script>
           </head>
           <body>
           <h1>F√≥rmula da Frequ√™ncia de Schroeder</h1>
           <p>A frequ√™ncia de Schroeder √© calculada usando a seguinte equa√ß√£o:</p>
           <p style="font-size: 15px; font-weight: bold;">
           \[ F_s = 2000 \times \sqrt{\frac{RT60}{V}} \]
           </p>
           <p>Onde:</p>
           <ul>
           <li><b>F<sub>s</sub></b>: requ√™ncia de Schroeder (Hz)</li>
           <li><b>RT60</b>: Tempo de reverbera√ß√£o (s)</li>
           <li><b>V</b>: Volume da sala (m¬≥)</li>
           </ul>
           </body>


            
           Ao final do processo de anota√ß√£o do conjunto de dados, obteve-se o seguintes r√≥tulos em .csv:

           <ul>
           <li>room_id</li>
           <li>File</li>
           <li>C50_125Hz</li>
           <li>C80_125Hz</li>
           <li>T60_T30_125Hz</li>
           <li>C50_250Hz</li>
           <li>C80_250Hz</li>
           <li>T60_T30_250Hz</li>
           <li>C50_500Hz</li>
           <li>C80_500Hz</li>
           <li>T60_T30_500Hz</li>
           <li>C50_1000Hz</li>
           <li>C80_1000Hz</li>
           <li>T60_T30_1000Hz</li>
           <li>C50_2000Hz</li>
           <li>C80_2000Hz</li>
           <li>T60_T30_2000Hz</li>
           <li>C50_4000Hz</li>
           <li>C80_4000Hz</li>
           <li>T60_T30_4000Hz</li>
           <li>Dimensions</li>
           <li>Temperature (¬∞C)</li>
           <li>Humidity</li>
           <li>√Årea Superficial (m¬≤)</li>
           <li>Volume (m¬≥)</li>
           <li>NRC</li>
           <li>Coeficiente_Absorcao_125_L_1</li>
           <li>Coeficiente_Absorcao_125_L_2</li>
           <li>Coeficiente_Absorcao_125_L_3</li>
           <li>Coeficiente_Absorcao_125_L_4</li>
           <li>Coeficiente_Absorcao_125_L_5</li>
           <li>Coeficiente_Absorcao_125_L_6</li>
           <li>Coeficiente_Absorcao_250_L_1</li>
           <li>Coeficiente_Absorcao_250_L_2</li>
           <li>Coeficiente_Absorcao_250_L_3</li>
           <li>Coeficiente_Absorcao_250_L_4</li>
           <li>Coeficiente_Absorcao_250_L_5</li>
           <li>Coeficiente_Absorcao_250_L_6</li>
           <li>Coeficiente_Absorcao_500_L_1</li>
           <li>Coeficiente_Absorcao_500_L_2</li>
           <li>Coeficiente_Absorcao_500_L_3</li>
           <li>Coeficiente_Absorcao_500_L_4</li>
           <li>Coeficiente_Absorcao_500_L_5</li>
           <li>Coeficiente_Absorcao_500_L_6</li>
           <li>Coeficiente_Absorcao_1000_L_1</li>
           <li>Coeficiente_Absorcao_1000_L_2</li>
           <li>Coeficiente_Absorcao_1000_L_3</li>
           <li>Coeficiente_Absorcao_1000_L_4</li>
           <li>Coeficiente_Absorcao_1000_L_5</li>
           <li>Coeficiente_Absorcao_1000_L_6</li>
           <li>Coeficiente_Absorcao_2000_L_1</li>
           <li>Coeficiente_Absorcao_2000_L_2</li>
           <li>Coeficiente_Absorcao_2000_L_3</li>
           <li>Coeficiente_Absorcao_2000_L_4</li>
           <li>Coeficiente_Absorcao_2000_L_5</li>
           <li>Coeficiente_Absorcao_2000_L_6</li>
           <li>Coeficiente_Absorcao_4000_L_1</li>
           <li>Coeficiente_Absorcao_4000_L_2</li>
           <li>Coeficiente_Absorcao_4000_L_3</li>
           <li>Coeficiente_Absorcao_4000_L_4</li>
           <li>Coeficiente_Absorcao_4000_L_5</li>
           <li>Coeficiente_Absorcao_4000_L_6</li>
           <li>room_name</li>
           </ul>

           <ul> 
           Sendo que a vari√°vel room_name √© a chave prim√°ria entre o arquivo .csv com os r√≥tulos e os tensores criados para a entrada no modelo RESNET-50.   
           </ul>
     

            


            
      






            
        
            Com o objetivo final de desenvolver uma abordagem para a estimativa cega de par√¢metros ac√∫sticos, este est√°gio inicial, de natureza experimental e explorat√≥ria, consistiu em implementar um modelo RESNET-50 para estimar as seguintes vari√°veis:
            </p>
            <ul>
               <li>√Årea Superficial (m¬≤)</li>
               <li>Volume (m¬≥)</li>
               <li>Superf√≠cie 1 (m¬≤)</li>
               <li>Superf√≠cie 2 (m¬≤)</li>
               <li>Superf√≠cie 3 (m¬≤)</li>
               <li>Superf√≠cie 4 (m¬≤)</li>
               <li>Superf√≠cie 5 (m¬≤)</li>
               <li>Superf√≠cie 6 (m¬≤)</li>
               <li>Absor√ß√£o M√©dia (125 Hz)</li>
               <li>Absor√ß√£o M√©dia (250 Hz)</li>
               <li>Absor√ß√£o M√©dia (500 Hz)</li>
               <li>Absor√ß√£o M√©dia (1000 Hz)</li>
               <li>Absor√ß√£o M√©dia (2000 Hz)</li>
               <li>Absor√ß√£o M√©dia (4000 Hz)</li>
               <li>T60_Sabine_125</li>
               <li>T60_Sabine_250</li>
               <li>T60_Sabine_500</li>
               <li>T60_Sabine_1000</li>
               <li>T60_Sabine_2000</li>
               <li>T60_Sabine_4000</li>
               <li>C50_125Hz</li>
               <li>T60_T30_125Hz</li>
               <li>C50_250Hz</li>
               <li>T60_T30_250Hz</li>
               <li>C50_500Hz</li>
               <li>T60_T30_500Hz</li>
               <li>C50_1000Hz</li>
               <li>T60_T30_1000Hz</li>
               <li>C50_2000Hz</li>
               <li>T60_T30_2000Hz</li>
               <li>C50_4000Hz</li>
               <li>T60_T30_4000Hz</li>
            </ul>
            
            <p>O resultado preliminar encontra-se no seguinte link: <a href="link-resultados" target="_blank">Resultados</a>.</p>
            <p>O modelo treinado encontra-se no seguinte link: <a href="link-modelo" target="_blank">Modelo Treinado</a>.</p>
            
            <h4>Caracter√≠sticas do Modelo</h4>
            <ul>
                <li>Base: ResNet-50 pr√©-treinada (ImageNet).</li>
                <li>Camadas finais removidas (mantendo apenas a extra√ß√£o de caracter√≠sticas).</li>
                <li>Camadas adicionais: 
                    <ul>
                        <li>Global Average Pooling (reduzindo a sa√≠da para 2048 neur√¥nios).</li>
                        <li>Uma cabe√ßa para predi√ß√£o dos valores esperados (mu_head).</li>
                        <li>Uma cabe√ßa para predi√ß√£o da incerteza (sigma_head), ativada com Softplus.</li>
                    </ul>
                </li>
            </ul>

            <h4>Fun√ß√£o de Perda - Log-Verossimilhan√ßa Gaussiana</h4>
            <p>
                O modelo utiliza uma fun√ß√£o de perda baseada em distribui√ß√£o Gaussiana para estimar a incerteza da predi√ß√£o. A equa√ß√£o utilizada √©:
            </p>
            <pre>
            L = 0.5 * Œ£ (log(œÉ¬≤) + ((y - Œº)¬≤ / œÉ¬≤))
            </pre>
            <p>Essa abordagem permite que o modelo aprenda n√£o apenas os valores das vari√°veis ac√∫sticas, mas tamb√©m o n√≠vel de incerteza associado √†s predi√ß√µes.</p>

            <h4>Hiperpar√¢metros do Treinamento</h4>
            <ul>
                <li>√âpocas: 50</li>
                <li>Otimizador: Adam (lr=1e-5)</li>
                <li>Divis√£o dos Dados: 80% Treino, 20% Valida√ß√£o</li>
                <li>Checkpoint: Salvo a cada √©poca</li>
                <li>GPU utilizada: NVIDIA T4</li>
            </ul>

            <h4>Treinamento e Valida√ß√£o</h4>
            <p>
                Durante o treinamento, o modelo foi avaliado em cada √©poca utilizando a fun√ß√£o de perda de log-verossimilhan√ßa Gaussiana. O progresso do treinamento e valida√ß√£o foi registrado, gerando um gr√°fico de perdas ao longo das √©pocas.
            </p>
            <ul>
                <li>Acur√°cia no treino: X%</li>
                <li>Erro de treino: Y</li>
                <li>Erro de valida√ß√£o: Z</li>
                <li>Acur√°cia no teste: W%</li>
                <li>Erro no teste: V</li>
            </ul>

            <h4>Visualiza√ß√£o das Perdas</h4>
            <p>O gr√°fico abaixo mostra a evolu√ß√£o da perda durante o treinamento:</p>
            <img src="link-grafico" alt="Gr√°fico de Treinamento e Valida√ß√£o" style="width: 100%; max-width: 600px;">
        </div>

        <!-- Espa√ßo reservado para Projeto 1 -->
        <div class="projeto">
            <h3><a href="link-projeto-1" target="_blank">Projeto 1</a></h3>
            <p>
                A ETAPA I: CONFIGURA√á√ÉO DOS AMBIENTES; ETAPA II: IMPLEMENTA√á√ÉO E TREINAMENTO DOS MODELOS; ETAPA III: TESTE DOS MODELOS; ETAPA IV: AVALIA√á√ÉO DAS M√âTRICAS DOS MODELOS.
            <ul>
               <li>ETAPA I: CONFIGURA√á√ÉO DOS AMBIENTES</li>
            </ul>  
            
            <ul>
               <li>PASSO 1: GERANDO AS CONFIGURA√á√ïES DA SALA</li>
            </ul>   
                Inicialmente, as dimens√µes das salas foram definidas aleatoriamente dentro de um intervalo preestabelecido. O comprimento e a largura variaram entre 3 e 12 metros, enquanto a altura assumiu valores entre 2,5 e 4,5 metros. Esses valores foram arredondados para uma casa decimal para garantir maior precis√£o nas simula√ß√µes.
                Al√©m das dimens√µes, foram atribu√≠dos coeficientes de difus√£o para cada superf√≠cie da sala. O mesmo valor foi aplicado uniformemente a todas as superf√≠cies e foi sorteado dentro de um intervalo entre 0,2 e 1. Esse coeficiente foi gerado para seis superf√≠cies distintas: quatro paredes, teto e ch√£o. Os valores foram armazenados de acordo com a frequ√™ncia correspondente, considerando seis faixas padronizadas: 125 Hz, 250 Hz, 500 Hz, 1000 Hz, 2000 Hz e 4000 Hz.
                Para determinar os coeficientes de absor√ß√£o, com o fim de definir perfis distintos para paredes, teto e ch√£o, foi utilizado um processo probabil√≠stico apresentado em  <a href="https://drive.google.com/file/d/11VNCIks217v6AZXI_0JtuiTwnnlTSDv-/view?usp=drive_link" download><button>Mean Absorption Estimation from Room Impulse Responses using Virtually-Supervised Learning</button></a></p> Em alguns casos, um √∫nico valor foi atribu√≠do a todas as superf√≠cies de um mesmo tipo, enquanto em outros, valores diferenciados foram gerados para cada frequ√™ncia. Esses coeficientes foram estabelecidos a partir de intervalos que variaram conforme o tipo de superf√≠cie, refletindo as propriedades ac√∫sticas de materiais comuns.
                As paredes puderam apresentar coeficientes de absor√ß√£o homog√™neos ou diferenciados por frequ√™ncia, variando entre valores baixos, t√≠picos de materiais reflexivos, e valores mais elevados, condizentes com superf√≠cies mais absorventes. O teto e o ch√£o seguiram uma l√≥gica semelhante, com intervalos de absor√ß√£o distintos de acordo com materiais t√≠picos desses elementos estruturais.
                Dessa forma, o c√≥digo gerou automaticamente um conjunto de par√¢metros realistas para diferentes configura√ß√µes de salas, permitindo a simula√ß√£o de ambientes ac√∫sticos diversos com base nas propriedades f√≠sicas de suas superf√≠cies.

             <ul>
               <li>PASSO 2: GERANDO AS CONFIGURA√á√ïES DOS RECEPTORES </li>
            </ul>
                A classe 'util_receiver' √© respons√°vel por definir e gerar arranjos de microfones em um ambiente tridimensional. Inicialmente, ela armazena os receptores em um dicion√°rio ('dic_receiver'). O m√©todo 'mic_defination_array' calcula as posi√ß√µes de dois microfones a partir de um ponto central (barycenter), aplicando uma matriz de rota√ß√£o para posicion√°-los a uma dist√¢ncia fixa de 22.5 cm. O m√©todo generate_receivers_rooms distribui aleatoriamente um conjunto de receptores dentro de uma sala, garantindo uma dist√¢ncia m√≠nima das paredes, al√©m de gerar √¢ngulos aleat√≥rios de orienta√ß√£o (yaw, pitch e roll). Ele utiliza o m√©todo 'rotation_matrix', que constr√≥i uma matriz de rota√ß√£o tridimensional com base nesses √¢ngulos, 
                permitindo a correta orienta√ß√£o dos microfones no espa√ßo. Os resultados incluem as posi√ß√µes dos microfones, os barycenters e os √¢ngulos de rota√ß√£o, tornando essa classe √∫til para simula√ß√µes ac√∫sticas e an√°lise de propaga√ß√£o sonora.

             <ul>
               <li>PASSO 3: GERANDO AS CONFIGURA√á√ïES DAS FONTES</li>
             </ul>
                A classe 'util_source' √© respons√°vel por gerar fontes sonoras dentro de uma sala tridimensional, garantindo posi√ß√µes seguras e evitando sobreposi√ß√£o com o barycenter. Ela cont√©m uma lista de descri√ß√µes de padr√µes de capta√ß√£o de microfones, incluindo op√ß√µes como omnidirecional, cardi√≥ide e bidirecional. O m√©todo 'generate_source_room' cria fontes reais na sala, garantindo que pelo menos uma delas seja omnidirecional 
                e posicionando as demais aleatoriamente com diferentes orienta√ß√µes (yaw, pitch e roll). O m√©todo 'fake_source_room' realiza um processo semelhante para fontes artificiais, garantindo uma dist√¢ncia m√≠nima entre elas e o barycenter. Ambos os m√©todos utilizam 'generate_safe_coordinates', que assegura que as fontes estejam dentro dos limites da sala e respeitem uma dist√¢ncia m√≠nima do barycenter antes de serem posicionadas. 
                Essa abordagem permite a simula√ß√£o de fontes sonoras em diferentes condi√ß√µes espaciais para an√°lise ac√∫stica.
            <ul>
               <li>PASSO 4: COMPILANDO AS INFORMA√á√ïES</li>
             </ul>
               A classe 'conf_files' combina funcionalidades das classes 'util_room', 'util_receiver' e 'util_source' para gerar arquivos de configura√ß√£o que descrevem salas ac√∫sticas, receptores (microfones) e fontes sonoras. No m√©todo '__init__', ela recebe o n√∫mero de salas e o nome do conjunto de dados, inicializando inst√¢ncias auxiliares e chamando os m√©todos 'params_file()' e 'room_file()'. O m√©todo 'params_file()' gera um arquivo YAML 
               contendo os par√¢metros da simula√ß√£o, como taxa de amostragem (48 kHz), ordem de reflex√£o, e n√∫mero de raios para o modelo de tra√ßado de raios difuso. O m√©todo 'room_file()' cria dicion√°rios com as propriedades de cada sala, incluindo dimens√µes, coeficientes de absor√ß√£o e difus√£o, al√©m da distribui√ß√£o de receptores e fontes sonoras. Para cada sala, ele posiciona cinco receptores garantindo uma dist√¢ncia m√≠nima das paredes, gera fontes reais e artificiais, 
               e armazena as configura√ß√µes em arquivos YAML separados para salas, receptores, fontes e fontes artificiais de ru√≠do. A abordagem facilita a simula√ß√£o e an√°lise ac√∫stica automatizada.
            
             <ul>
               <li>PASSO 5: GERANDO E ARMAZENADOS AS SALAS</li>
             </ul>
               A chamada 'conf_files(1000, "test")' cria e configura automaticamente 1000 salas ac√∫sticas para simula√ß√£o, gerando arquivos YAML detalhados com par√¢metros da sala, receptores e fontes sonoras. Durante a execu√ß√£o, a classe define dimens√µes aleat√≥rias para cada sala, calcula coeficientes de absor√ß√£o e difus√£o, posiciona cinco receptores respeitando uma dist√¢ncia m√≠nima das paredes e distribui fontes sonoras reais e artificiais, garantindo ao menos uma omnidirecional. 
               Os dados s√£o organizados em arquivos YAML, incluindo conf_sim_params.yml para par√¢metros da simula√ß√£o, conf_room_setup_2.yml com detalhes das salas, conf_receivers_2.yml para microfones, conf_source_2.yml para fontes reais e conf_noise_source_2.yml para fontes artificiais, permitindo sua aplica√ß√£o em estudos de propaga√ß√£o sonora e modelagem ac√∫stica.
            
    </section>

    


    <section id="api-estimacao">
        <h2>API para Estima√ß√£o</h2>
             <p>Esta API fornece funcionalidades avan√ßadas para estimar par√¢metros ac√∫sticos e realizar processamento de sinais.</p>
             <p>Acesse a API clicando no link abaixo:</p>
             <a href="https://seu-link-da-api.com" target="_blank">Acesse a API para Estima√ß√£o</a>
    </section>


    

  
    
    <section id="artigos">
        <h2>Artigos Publicados</h2>
        <div class="artigo">
            <h3>Unreal Engine 5 Simulations of Solar Plant Inspections by Unmanned Aerial Systems with Robot Operating System 2</h3>
            <p><strong>Autores:</strong> Fabio Andrade, Agnar Sivertsen, Marcos G L Moura, Lucas Cavalcante Clarino, Gabriel Souza Machado Gonzal√©z, Luis Paulo Albuquerque Guedes, Carlos Alberto Correia, Mariane Petraglia, Alessandro Rosa Lopes Zachi </p>
            <p><strong>Resumo:</strong> Este artigo apresenta um sistema de simula√ß√£o de alta fidelidade para inspe√ß√£o de usinas solares utilizando Sistemas A√©reos N√£o Tripulados. Ele integra o Unreal Engine 5 para visuais realistas, o AirSim para simula√ß√£o da f√≠sica e sensores de drones e o Robot Operating System 2 para desenvolvimento de algoritmos. O sistema permite testar algoritmos de inspe√ß√£o em um ambiente virtual realista, aprimorando o desenvolvimento de solu√ß√µes de vis√£o computacional e detec√ß√£o de objetos. Um estudo de caso utilizando a Unidade de Controle de Voo ArduPilot, Detec√ß√£o de Bordas de Canny e controle PID demonstra sua efic√°cia.</p>
            <p><a href="link-artigo" target="_blank">Acesse o artigo completo</a></p>
        </div>
        
        <div class="artigo">
            <h3>Radio Frequency-Audio Based Drone Classification using Deep Learning Methods</h3>
            <p><strong>Autores:</strong> Luis Paulo Albuquerque Guedes, Mariane Petraglia, R√™mulo M. Caminha Gomes</p>
            <p><strong>Resumo:</strong> Este artigo prop√µe um modelo de aprendizado profundo que utiliza a fus√£o de dados de sensores ac√∫sticos e de radiofrequ√™ncia (RF) para a classifica√ß√£o de Ve√≠culos A√©reos N√£o Tripulados (UAVs). O modelo visa aumentar a precis√£o na classifica√ß√£o de diferentes tipos de drones, garantindo redund√¢ncia para evitar falhas. O trabalho foi estruturado em tr√™s etapas: an√°lise ac√∫stica para classifica√ß√£o de tipos de drones (etapa 1), classifica√ß√£o baseada em assinaturas de RF (etapa 2) e integra√ß√£o dos sensores (etapa 3). A combina√ß√£o de sensores ac√∫sticos e de RF resultou em uma melhoria significativa no desempenho da classifica√ß√£o de drones, aumentando a precis√£o de 0.8342 para 0.8617. Al√©m disso, o uso dessa fus√£o de dados melhorou a precis√£o em at√© 15,3% em faixas de SNR mais baixas.</p>
            <p><a href="link-artigo" target="_blank">Acesse o artigo completo</a></p>
        </div>
        
        <div class="artigo">
            <h3>An Integrated Framework for UAV Sound Tracking and Classification Using Deep Learning Techniques and Kalman Filters</h3>
            <p><strong>Autores:</strong> Luis Paulo Albuquerque Guedes, Mariane Petraglia, Pedro Henrique Monteiro Guedes</p>
            <p><strong>Resumo:</strong> Este artigo apresenta um framework integrado que combina classifica√ß√£o ac√∫stica e rastreamento de trajet√≥ria de Ve√≠culos A√©reos N√£o Tripulados (UAVs) utilizando t√©cnicas de aprendizado profundo e filtros de Kalman. A abordagem proposta visa melhorar a precis√£o na identifica√ß√£o do tipo de UAV e na predi√ß√£o de trajet√≥ria, com base em dados ac√∫sticos passivos reais registrados em uma c√¢mara anecoica e convolvidos com ru√≠do ambiental. Os resultados das simula√ß√µes confirmaram a efic√°cia do sistema tanto nas tarefas de classifica√ß√£o quanto na estimativa de trajet√≥ria. Para a classifica√ß√£o do tipo de UAV, o F1-score geral atingiu 0.8342, com o Drone C obtendo o melhor desempenho (0.8504). Para a classifica√ß√£o da dire√ß√£o de manobra, os Drones A e B alcan√ßaram um F1-score de 0.8277, enquanto o Drone C obteve 0.8298. O erro m√©dio de dist√¢ncia na estimativa de trajet√≥ria foi de aproximadamente 8 metros, com desvios padr√£o variando de 3.34 a 3.55 metros, demonstrando consist√™ncia e precis√£o em todas as dimens√µes avaliadas.</p>
            <p><a href="link-artigo" target="_blank">Acesse o artigo completo</a></p>
        </div>

     </section>

     <section id="livros">
        <h2>Livros Recomendados</h2>
        <p>Aqui est√£o alguns livros recomendados sobre modelagem ac√∫stica e processamento de sinais:</p>
        <ul>
            <li><a href="link-livro-1" target="_blank">Livro 1</a></li>
            <li><a href="link-livro-2" target="_blank">Livro 2</a></li>
            <li><a href="link-livro-3" target="_blank">Livro 3</a></li>
        <!-- Adicione mais livros conforme necess√°rio -->
        </ul>
     </section>
    

     <section id="contato">
        <h2>Contato</h2>
        <ul>
            <li>
                <img src="email-icon.png" alt="Email" width="20"> 
                <a href="mailto:luis.albuquerque@marinha.mil.br">luis.albuquerque@marinha.mil.br</a>
            </li>
            <li>
                <img src="email-icon.png" alt="Email" width="20"> 
                <a href="mailto:luis.guedes@coppe.ufrj.br">luis.guedes@coppe.ufrj.br</a>
            </li>
            <li>
                <img src="email-icon.png" alt="Email" width="20"> 
                <a href="mailto:luispauloaguedes@gmail.com">luispauloaguedes@gmail.com</a>
            </li>
            <li>
                <img src="github-icon.png" alt="GitHub" width="20"> 
                <a href="https://github.com/seuusuario" target="_blank">GitHub</a>
            </li>
            <li>
                <img src="linkedin-icon.png" alt="LinkedIn" width="20"> 
                <a href="https://www.linkedin.com/in/seuperfil" target="_blank">LinkedIn</a>
            </li>
        </ul>
    </section>

    <script>
        function translate(language) {
            if (language === 'en') {
                window.location.href = "pagina-ingles.html";
            } else if (language === 'de') {
                window.location.href = "pagina-alemao.html";
            }
        }
    </script>

    <script>
        function translate(language) {
            if (language === 'en') {
                window.location.href = "pagina-ingles.html";  // P√°gina em ingl√™s
            } else if (language === 'de') {
                window.location.href = "pagina-alemao.html";  // P√°gina em alem√£o
            }
        }
    </script>

</body>
</html>
